{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# !pip install pandas\n",
    "# !pip install --upgrade kagglehub\n",
    "# !pip install -U LibRecommender\n",
    "# !pip install keras==2.12.0 tensorflow==2.12.0\n",
    "#\n",
    "# !pip show LibRecommender"
   ],
   "metadata": {
    "id": "gS5ZkBCrWQOY",
    "ExecuteTime": {
     "end_time": "2025-03-29T16:50:03.928362Z",
     "start_time": "2025-03-29T16:50:03.925693Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kDA15rXbWIoB",
    "ExecuteTime": {
     "end_time": "2025-03-29T16:51:48.801747Z",
     "start_time": "2025-03-29T16:51:48.783080Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from libreco.data import random_split, DatasetPure\n",
    "from libreco.algorithms import BPR, UserCF, ItemCF, SVD, SVDpp, ALS\n",
    "from libreco.evaluation import evaluate\n",
    "import kagglehub\n",
    "import tensorflow as tf\n",
    "\n",
    "class RecipeRecommender:\n",
    "    def __init__(self, data_path=\"shuyangli94/food-com-recipes-and-user-interactions\"):\n",
    "        # Hyperparameter mit BPR als Referenz\n",
    "        self.embed_size = 64     # Für Embedding-basierte Modelle\n",
    "        self.n_epochs = 8         # Für trainierbare Modelle\n",
    "        self.lr = 5e-5            # Lernrate\n",
    "        self.reg = 5e-6           # Regularisierung\n",
    "        self.batch_size = 1024    # Batch-Größe\n",
    "        self.num_neg = 10          # Negative Samples\n",
    "        self.sampler = \"random\"   # Sampling-Methode\n",
    "        self.k_sim = 50           # Für CF-Modelle\n",
    "        self.sim_type = \"cosine\"  # Ähnlichkeitsmaß\n",
    "\n",
    "        # Gemeinsame Objekte\n",
    "        self.model = None\n",
    "        self.data_info = None\n",
    "        self.name_df = None\n",
    "        self.data_filtered = None\n",
    "        self.train_data = None\n",
    "        self.eval_data = None\n",
    "        self.test_data = None\n",
    "        self.user_id_map = {}\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self._load_recipe_names()\n",
    "\n",
    "\n",
    "    def set_model(self, model_type):\n",
    "        \"\"\"Zentrale Methode zur Modellauswahl mit einheitlichen Parametern\"\"\"\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        self.__prepare_data(model_type)  # Daten für alle Modelle vorbereiten\n",
    "\n",
    "        common_params = {\n",
    "            \"task\": \"ranking\",\n",
    "            \"data_info\": self.data_info\n",
    "        }\n",
    "\n",
    "        model_config = {\n",
    "           \"BPR\": {\n",
    "            \"class\": BPR,\n",
    "            \"params\": {\n",
    "                \"loss_type\": \"bpr\",\n",
    "                \"embed_size\": self.embed_size,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"lr\": self.lr,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"num_neg\": self.num_neg,\n",
    "                \"reg\": self.reg,\n",
    "                \"sampler\": self.sampler\n",
    "            }\n",
    "        },\n",
    "        \"UserCF\": {\n",
    "            \"class\": UserCF,\n",
    "            \"params\": {\n",
    "                \"k_sim\": self.k_sim,\n",
    "                \"sim_type\": self.sim_type\n",
    "            }\n",
    "        },\n",
    "        \"ItemCF\": {\n",
    "            \"class\": ItemCF,\n",
    "            \"params\": {\n",
    "                \"k_sim\": self.k_sim,\n",
    "                \"sim_type\": self.sim_type\n",
    "            }\n",
    "        },\n",
    "        \"SVD\": {\n",
    "            \"class\": SVD,\n",
    "            \"params\": {\n",
    "                \"embed_size\": self.embed_size,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"lr\": self.lr,\n",
    "                \"reg\": self.reg\n",
    "            }\n",
    "        },\n",
    "        \"SVDpp\": {\n",
    "            \"class\": SVDpp,\n",
    "            \"params\": {\n",
    "                \"embed_size\": self.embed_size,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"lr\": self.lr,\n",
    "                \"reg\": self.reg,\n",
    "            }\n",
    "        },\n",
    "        \"ALS\": {\n",
    "            \"class\": ALS,\n",
    "            \"params\": {\n",
    "                \"embed_size\": self.embed_size,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"reg\": self.reg,\n",
    "                \"alpha\": 10,\n",
    "                \"use_cg\": True,\n",
    "                \"n_threads\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "        config = model_config.get(model_type)\n",
    "        if not config:\n",
    "            raise ValueError(f\"Unbekanntes Modell: {model_type}\")\n",
    "\n",
    "        self.model = config[\"class\"](**common_params, **config[\"params\"])\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "      if not self.model:\n",
    "          raise ValueError(\"Model not trained. Call set_model() first.\")\n",
    "\n",
    "     # Gemeinsame Parameter\n",
    "      common_params = {\n",
    "          \"verbose\": 2,\n",
    "          \"eval_data\": self.eval_data,\n",
    "          \"metrics\": [\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"]\n",
    "     }\n",
    "\n",
    "      # Modellspezifische Parameter\n",
    "      if isinstance(self.model, (UserCF, ItemCF)):\n",
    "          # Für Collaborative Filtering\n",
    "          fit_params = {\n",
    "              \"neg_sampling\": True,\n",
    "              \"verbose\": 1\n",
    "         }\n",
    "      else:\n",
    "          # Für Embedding-basierte Modelle: batch_size entfernen\n",
    "          fit_params = {\n",
    "              \"neg_sampling\": True,\n",
    "              \"shuffle\": True,\n",
    "             **common_params\n",
    "         }\n",
    "\n",
    "     # Training durchführen\n",
    "      self.model.fit(\n",
    "          self.train_data,\n",
    "          **fit_params\n",
    "      )\n",
    "\n",
    "    def load_and_preprocess(self, min_interactions):\n",
    "        \"\"\"Load and preprocess interaction data\"\"\"\n",
    "        # Download and load dataset\n",
    "        path = kagglehub.dataset_download(self.data_path)\n",
    "\n",
    "        # Load and combine interaction data\n",
    "        train = pd.read_csv(os.path.join(path, \"interactions_train.csv\"))\n",
    "        eval = pd.read_csv(os.path.join(path, \"interactions_validation.csv\"))\n",
    "        test = pd.read_csv(os.path.join(path, \"interactions_test.csv\"))\n",
    "\n",
    "        combined = pd.concat([train, eval, test], ignore_index=True)\n",
    "        combined = self._rename_and_filter_data(combined)\n",
    "\n",
    "        # Filter items\n",
    "        item_counts = combined[\"item\"].value_counts()\n",
    "        items_to_keep = item_counts[item_counts >= min_interactions].index\n",
    "        filtered = combined[combined[\"item\"].isin(items_to_keep)]\n",
    "\n",
    "        # Filter users\n",
    "        user_counts = filtered[\"user\"].value_counts()\n",
    "        users_to_keep = user_counts[user_counts >= min_interactions].index\n",
    "        self.data_filtered = filtered[filtered[\"user\"].isin(users_to_keep)]\n",
    "\n",
    "    def __prepare_data(self,model_type):\n",
    "      # Convert ratings to 0/1 for UserCF and ItemCF\n",
    "        if model_type in [\"UserCF\", \"ItemCF\"]:\n",
    "        # Binarize ratings: 0-2 → 0, 3-5 → 1\n",
    "          self.data_filtered['label'] = self.data_filtered['label'].apply(\n",
    "              lambda x: 0 if x <= 2 else 1\n",
    "          )\n",
    "      # Split data\n",
    "        self.train_data, self.eval_data, self.test_data = random_split(\n",
    "            self.data_filtered,\n",
    "            multi_ratios=[0.8, 0.1, 0.1]\n",
    "        )\n",
    "\n",
    "        # Build datasets\n",
    "        self.train_data, self.data_info = DatasetPure.build_trainset(self.train_data)\n",
    "        self.eval_data = DatasetPure.build_evalset(self.eval_data)\n",
    "        self.test_data = DatasetPure.build_testset(self.test_data)\n",
    "\n",
    "\n",
    "    def save_recommendations_as_csv(self,items_information,amount_of_recs, path):\n",
    "      df = self.get_recommendations(items_information,amount_of_recs)\n",
    "      df.to_csv(path, index=False)\n",
    "      return df\n",
    "\n",
    "    def get_recommendations(self, items_information, n_rec):\n",
    "        \"\"\"\n",
    "        Holt Empfehlungen für alle User in user_id_map und speichert die Ergebnisse in einem DataFrame.\n",
    "        \"\"\"\n",
    "        dfs = []\n",
    "        for user_identifier in self.user_id_map:\n",
    "            df = self.get_recommendation(user_identifier, n_rec, items_information)\n",
    "            dfs.append(df)\n",
    "        # Alle einzelnen DataFrames zusammenfügen\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        return final_df\n",
    "\n",
    "    def get_recommendation(self, user_identifier, n_rec, items_information):\n",
    "      \"\"\"Get recommendations for a user (UUID or numeric ID) und speichert alle Daten in einem DataFrame\"\"\"\n",
    "      if not self.model:\n",
    "          raise ValueError(\"Model not trained. Call train() first.\")\n",
    "\n",
    "      # UUID Lookup\n",
    "      if isinstance(user_identifier, str):\n",
    "          if user_identifier not in self.user_id_map:\n",
    "              raise ValueError(f\"User UUID '{user_identifier}' not found.\")\n",
    "          user_id = self.user_id_map[user_identifier]\n",
    "\n",
    "     # Empfehlungen abrufen\n",
    "      recommendations = self.model.recommend_user(\n",
    "          user=user_id,\n",
    "          n_rec=n_rec,\n",
    "          filter_consumed=True\n",
    "     )\n",
    "\n",
    "      # Liste für die Daten vorbereiten\n",
    "      records = []\n",
    "      for recipe in recommendations[user_id]:\n",
    "          # Item-Titel und Zutaten anhand der recipe_id abrufen\n",
    "          item_title, item_ingredients = self.__find_item_by_id(recipe, items_information)\n",
    "          # Datensatz zur Liste hinzufügen\n",
    "          records.append({\n",
    "              \"uuid\": user_identifier,\n",
    "              \"item_id\": recipe,\n",
    "             \"item_title\": item_title,\n",
    "              \"item_ingredients\": item_ingredients\n",
    "          })\n",
    "\n",
    "      # DataFrame aus der Liste erstellen\n",
    "      df = pd.DataFrame(records)\n",
    "      return df\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        return evaluate(\n",
    "            model=self.model,\n",
    "            data=self.test_data,\n",
    "            neg_sampling=True,\n",
    "            metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"]\n",
    "        )\n",
    "\n",
    "    def _load_recipe_names(self):\n",
    "        \"\"\"Load recipe ID to name mapping\"\"\"\n",
    "        path = kagglehub.dataset_download(self.data_path)\n",
    "        raw_recipes_path = os.path.join(path, \"RAW_recipes.csv\")\n",
    "        self.name_df = pd.read_csv(raw_recipes_path)[[\"name\", \"id\"]]\n",
    "\n",
    "    def _rename_and_filter_data(self, interactions_data):\n",
    "      # Erzeuge explizite Kopie des DataFrames\n",
    "      df = interactions_data.copy()\n",
    "\n",
    "      # Spalten umbenennen (ohne inplace)\n",
    "      df = df.rename(columns={\n",
    "          \"user_id\": \"user\",\n",
    "          \"recipe_id\": \"item\",\n",
    "          \"rating\": \"label\"\n",
    "      })\n",
    "\n",
    "      # Spalten filtern\n",
    "      keep_cols = [\"user\", \"item\", \"label\"]\n",
    "      df = df[keep_cols]\n",
    "\n",
    "      # Typkonvertierung mit .loc\n",
    "      df.loc[:, \"label\"] = df[\"label\"].astype(int)\n",
    "      return df\n",
    "\n",
    "    def _get_recipe_name(self, recipe_id):\n",
    "        \"\"\"Helper to get recipe name from ID\"\"\"\n",
    "        name = self.name_df.loc[self.name_df['id'] == recipe_id, 'name']\n",
    "        return name.values[0] if not name.empty else \"Unknown Recipe\"\n",
    "\n",
    "    def import_ratings_csv(self, file_path):\n",
    "        \"\"\"Import ratings from CSV and map recipe names to correct IDs\"\"\"\n",
    "        try:\n",
    "            # Load CSV\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(\"CSV erfolgreich geladen:\")\n",
    "            print(df.head())\n",
    "\n",
    "            # Check if recipe_name column exists\n",
    "            if \"item_title\" not in df.columns:\n",
    "                raise ValueError(\"Fehlende Spalte: item_title\")\n",
    "\n",
    "            # Map recipe names to correct IDs\n",
    "            df[\"item_id\"] = df[\"item_title\"].apply(lambda name: self.__find_item_id_by_name(name))\n",
    "\n",
    "            # Check required columns after mapping\n",
    "            required = {\"uuid\", \"item_id\", \"rating\"}\n",
    "            if not required.issubset(df.columns):\n",
    "                missing = required - set(df.columns)\n",
    "                raise ValueError(f\"Fehlende Spalten: {missing}\")\n",
    "\n",
    "            # Process and map UUIDs\n",
    "            processed_df = self.__process_ratings(df)\n",
    "\n",
    "            # Add to data\n",
    "            self.data_filtered = pd.concat(\n",
    "                [self.data_filtered, processed_df],\n",
    "                ignore_index=True\n",
    "            )\n",
    "            print(f\"{len(processed_df)} neue Bewertungen hinzugefügt.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Importieren der Bewertungen: {e}\")\n",
    "\n",
    "    def __process_ratings(self, df):\n",
    "      \"\"\"Map UUIDs to numeric IDs\"\"\"\n",
    "      # Rename columns\n",
    "      df = df.rename(columns={\n",
    "          \"uuid\": \"user\",\n",
    "          \"item_id\": \"item\",\n",
    "          \"rating\": \"label\"\n",
    "      })\n",
    "\n",
    "      # Convert from range [-2,2] to [1,5]\n",
    "      df[\"label\"] = df[\"label\"] + 3\n",
    "\n",
    "      # Determine current max ID from user_id_map\n",
    "      current_max = max(self.user_id_map.values()) if self.user_id_map else 0\n",
    "\n",
    "      # Generate new IDs for unknown UUIDs\n",
    "      new_users = [uuid for uuid in df[\"user\"].unique() if uuid not in self.user_id_map]\n",
    "      num_new = len(new_users)\n",
    "\n",
    "      print(\"WTH\", num_new)\n",
    "      if num_new > 0:\n",
    "          new_ids = range(current_max + 1, current_max + num_new + 1)\n",
    "          self.user_id_map.update(zip(new_users, new_ids))\n",
    "\n",
    "      # Replace UUIDs with numeric IDs\n",
    "      df[\"user\"] = df[\"user\"].map(self.user_id_map)\n",
    "      return df\n",
    "\n",
    "\n",
    "    def __get_score(self,userid,itemid):\n",
    "     return self.model.predict(userid,itemid)\n",
    "\n",
    "    def __find_item_by_id(self,recipe_id, items_information):\n",
    "      df = items_information.loc[items_information[\"id\"] == recipe_id]\n",
    "      return df['name'].values[0], df['ingredients'].values[0]\n",
    "\n",
    "    def __find_item_id_by_name(self, item_name):\n",
    "      df = self.name_df.loc[self.name_df[\"name\"] == item_name]\n",
    "      return df['id'].values[0]"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "def load_items_information():\n",
    "    path = kagglehub.dataset_download(\"shuyangli94/food-com-recipes-and-user-interactions\")\n",
    "\n",
    "    recipes_path = os.path.join(path, \"RAW_recipes.csv\")\n",
    "    recipes = pd.read_csv(recipes_path)\n",
    "\n",
    "    return recipes\n",
    "\n",
    "items_information = load_items_information()"
   ],
   "metadata": {
    "id": "QWp6kdNpY1zs",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "outputId": "3670a94f-13e3-4594-a013-e49013e03461",
    "ExecuteTime": {
     "end_time": "2025-03-29T16:50:10.075066Z",
     "start_time": "2025-03-29T16:50:07.116585Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ou5fJq46b9og",
    "outputId": "5a44f197-3cf7-48b9-a9d3-65df0c12acc0",
    "ExecuteTime": {
     "end_time": "2025-03-29T16:54:39.307506Z",
     "start_time": "2025-03-29T16:51:50.082849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import contextlib\n",
    "from config import RATINGS_FILE\n",
    "from config import RECOMMENDATIONS_FILE\n",
    "import io\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "RECOMMENDER_TYPES = [\"SVD\", \"SVDpp\", \"ALS\", \"BPR\", \"UserCF\", \"ItemCF\"]\n",
    "\n",
    "for RECOMMENDER_TYPE in RECOMMENDER_TYPES:\n",
    "    print(f\"Training {RECOMMENDER_TYPE}...\")\n",
    "\n",
    "    with contextlib.redirect_stdout(io.StringIO()): # suppresses print statements\n",
    "        recommender = RecipeRecommender()\n",
    "        recommender.load_and_preprocess(min_interactions=20)\n",
    "        # Neue Nutzer per CSV importieren\n",
    "        recommender.import_ratings_csv(\"../\"+RATINGS_FILE)\n",
    "        recommender.set_model(RECOMMENDER_TYPE)\n",
    "        recommender.train()\n",
    "        eval = recommender.evaluate()\n",
    "\n",
    "    print(eval)\n",
    "    recommendations = recommender.save_recommendations_as_csv(\n",
    "        items_information,\n",
    "        20,\n",
    "        \"../\"+RECOMMENDATIONS_FILE+RECOMMENDER_TYPE.lower()+\".csv\"\n",
    "    )\n",
    "\n",
    "    print(f\"Recommendations for {RECOMMENDER_TYPE} saved to {RECOMMENDATIONS_FILE+RECOMMENDER_TYPE.lower()+'.csv'}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 826/826 [00:01<00:00, 591.04it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 834.56it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4423.23it/s]\n",
      "train: 100%|██████████| 826/826 [00:01<00:00, 651.02it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 461.29it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3785.18it/s]\n",
      "train: 100%|██████████| 826/826 [00:01<00:00, 625.56it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 695.20it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3943.88it/s]\n",
      "train: 100%|██████████| 826/826 [00:01<00:00, 628.80it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 683.03it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4122.42it/s]\n",
      "train: 100%|██████████| 826/826 [00:01<00:00, 635.97it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 590.31it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4316.91it/s]\n",
      "train: 100%|██████████| 826/826 [00:01<00:00, 641.13it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 771.05it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4108.03it/s]\n",
      "train: 100%|██████████| 826/826 [00:01<00:00, 659.00it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 692.67it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3656.16it/s]\n",
      "train: 100%|██████████| 826/826 [00:01<00:00, 618.57it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 592.77it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 2711.77it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 482.24it/s]\n",
      "eval_listwise: 100%|██████████| 2401/2401 [00:00<00:00, 3731.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6878232019150221, 'roc_auc': 0.668207986371163, 'precision': 0.017284464806330696, 'recall': 0.03666426184226875, 'ndcg': 0.07620526905902279}\n",
      "Recommendations for SVD saved to data/recommendations_svd.csv\n",
      "Training SVDpp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 826/826 [00:06<00:00, 133.50it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 905.70it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4255.35it/s]\n",
      "train: 100%|██████████| 826/826 [00:05<00:00, 140.94it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 711.26it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4361.41it/s]\n",
      "train: 100%|██████████| 826/826 [00:05<00:00, 143.24it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 632.77it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4484.68it/s]\n",
      "train: 100%|██████████| 826/826 [00:05<00:00, 138.54it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 753.08it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3649.20it/s]\n",
      "train: 100%|██████████| 826/826 [00:06<00:00, 135.57it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 581.01it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 2573.49it/s]\n",
      "train: 100%|██████████| 826/826 [00:05<00:00, 139.03it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 613.85it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3611.27it/s]\n",
      "train: 100%|██████████| 826/826 [00:05<00:00, 138.15it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 622.65it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3642.84it/s]\n",
      "train: 100%|██████████| 826/826 [00:05<00:00, 139.06it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 656.33it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4053.65it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 534.85it/s]\n",
      "eval_listwise: 100%|██████████| 2401/2401 [00:00<00:00, 3691.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6711130683806029, 'roc_auc': 0.6747311730511194, 'precision': 0.019241982507288632, 'recall': 0.04455498260414667, 'ndcg': 0.08357291029879352}\n",
      "Recommendations for SVDpp saved to data/recommendations_svdpp.csv\n",
      "Training ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 419.58it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3567.48it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 768.61it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3871.51it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 769.56it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3309.77it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 561.71it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3427.79it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 781.83it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4442.23it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 809.55it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 2986.23it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 883.48it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3493.71it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 813.84it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3421.31it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 460.95it/s]\n",
      "eval_listwise: 100%|██████████| 2401/2401 [00:00<00:00, 3352.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6791907465111139, 'roc_auc': 0.614718052331181, 'precision': 0.012078300708038319, 'recall': 0.030527470585607924, 'ndcg': 0.0547997478245302}\n",
      "Recommendations for ALS saved to data/recommendations_als.csv\n",
      "Training BPR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1036/1036 [00:01<00:00, 588.59it/s]\n",
      "eval_pointwise: 100%|██████████| 18/18 [00:00<00:00, 940.99it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4051.69it/s]\n",
      "train: 100%|██████████| 1036/1036 [00:01<00:00, 596.11it/s]\n",
      "eval_pointwise: 100%|██████████| 18/18 [00:00<00:00, 738.73it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4147.85it/s]\n",
      "train: 100%|██████████| 1036/1036 [00:01<00:00, 637.88it/s]\n",
      "eval_pointwise: 100%|██████████| 18/18 [00:00<00:00, 850.59it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3756.24it/s]\n",
      "train: 100%|██████████| 1036/1036 [00:01<00:00, 643.74it/s]\n",
      "eval_pointwise: 100%|██████████| 18/18 [00:00<00:00, 775.96it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4042.85it/s]\n",
      "train: 100%|██████████| 1036/1036 [00:01<00:00, 656.11it/s]\n",
      "eval_pointwise: 100%|██████████| 18/18 [00:00<00:00, 780.09it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3528.72it/s]\n",
      "train: 100%|██████████| 1036/1036 [00:01<00:00, 609.58it/s]\n",
      "eval_pointwise: 100%|██████████| 18/18 [00:00<00:00, 638.26it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 4229.25it/s]\n",
      "train: 100%|██████████| 1036/1036 [00:01<00:00, 609.60it/s]\n",
      "eval_pointwise: 100%|██████████| 18/18 [00:00<00:00, 625.19it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3441.91it/s]\n",
      "train: 100%|██████████| 1036/1036 [00:01<00:00, 641.28it/s]\n",
      "eval_pointwise: 100%|██████████| 18/18 [00:00<00:00, 851.66it/s]\n",
      "eval_listwise: 100%|██████████| 2425/2425 [00:00<00:00, 3533.87it/s]\n",
      "eval_pointwise: 100%|██████████| 18/18 [00:00<00:00, 875.45it/s]\n",
      "eval_listwise: 100%|██████████| 2401/2401 [00:00<00:00, 3621.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6862985082920336, 'roc_auc': 0.6609456010066809, 'precision': 0.01657642648896293, 'recall': 0.03274216763774256, 'ndcg': 0.07328320480665594}\n",
      "Recommendations for BPR saved to data/recommendations_bpr.csv\n",
      "Training UserCF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "top_k: 100%|██████████| 2511/2511 [00:00<00:00, 7325.86it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00,  8.77it/s]\n",
      "eval_listwise: 100%|██████████| 2396/2396 [00:08<00:00, 291.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.887533769781554, 'roc_auc': 0.5814682716197465, 'precision': 0.019240400667779635, 'recall': 0.043639376101859806, 'ndcg': 0.08812397966194815}\n",
      "Recommendations for UserCF saved to data/recommendations_usercf.csv\n",
      "Training ItemCF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "top_k: 100%|██████████| 4527/4527 [00:00<00:00, 5317.73it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00,  8.96it/s]\n",
      "eval_listwise: 100%|██████████| 2396/2396 [00:06<00:00, 344.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.869423674388294, 'roc_auc': 0.4945897836628138, 'precision': 0.01373121869782972, 'recall': 0.034119062135645986, 'ndcg': 0.05512266287355322}\n",
      "Recommendations for ItemCF saved to data/recommendations_itemcf.csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T16:50:33.932732184Z",
     "start_time": "2025-03-28T12:54:39.822374Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
