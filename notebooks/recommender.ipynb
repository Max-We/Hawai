{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# !pip install pandas\n",
    "# !pip install --upgrade kagglehub\n",
    "# !pip install -U LibRecommender\n",
    "# !pip install keras==2.12.0 tensorflow==2.12.0\n",
    "#\n",
    "# !pip show LibRecommender"
   ],
   "metadata": {
    "id": "gS5ZkBCrWQOY",
    "ExecuteTime": {
     "end_time": "2025-03-28T12:12:15.533087Z",
     "start_time": "2025-03-28T12:12:15.528124Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kDA15rXbWIoB",
    "ExecuteTime": {
     "end_time": "2025-03-28T12:12:15.668740Z",
     "start_time": "2025-03-28T12:12:15.608584Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from libreco.data import random_split, DatasetPure\n",
    "from libreco.algorithms import BPR, UserCF, ItemCF, SVD, SVDpp, ALS\n",
    "from libreco.evaluation import evaluate\n",
    "import kagglehub\n",
    "import tensorflow as tf\n",
    "\n",
    "class RecipeRecommender:\n",
    "    def __init__(self, data_path=\"shuyangli94/food-com-recipes-and-user-interactions\"):\n",
    "        # Hyperparameter mit BPR als Referenz\n",
    "        self.embed_size = 256     # Für Embedding-basierte Modelle\n",
    "        self.n_epochs = 5         # Für trainierbare Modelle\n",
    "        self.lr = 5e-5            # Lernrate\n",
    "        self.reg = 5e-6           # Regularisierung\n",
    "        self.batch_size = 1024    # Batch-Größe\n",
    "        self.num_neg = 5          # Negative Samples\n",
    "        self.sampler = \"random\"   # Sampling-Methode\n",
    "        self.k_sim = 50           # Für CF-Modelle\n",
    "        self.sim_type = \"cosine\"  # Ähnlichkeitsmaß\n",
    "\n",
    "        # Gemeinsame Objekte\n",
    "        self.model = None\n",
    "        self.data_info = None\n",
    "        self.name_df = None\n",
    "        self.data_filtered = None\n",
    "        self.train_data = None\n",
    "        self.eval_data = None\n",
    "        self.test_data = None\n",
    "        self.user_id_map = {}\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self._load_recipe_names()\n",
    "\n",
    "\n",
    "    def set_model(self, model_type):\n",
    "        \"\"\"Zentrale Methode zur Modellauswahl mit einheitlichen Parametern\"\"\"\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        self.__prepare_data(model_type)  # Daten für alle Modelle vorbereiten\n",
    "\n",
    "        common_params = {\n",
    "            \"task\": \"ranking\",\n",
    "            \"data_info\": self.data_info\n",
    "        }\n",
    "\n",
    "        model_config = {\n",
    "           \"BPR\": {\n",
    "            \"class\": BPR,\n",
    "            \"params\": {\n",
    "                \"loss_type\": \"bpr\",\n",
    "                \"embed_size\": self.embed_size,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"lr\": self.lr,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"num_neg\": self.num_neg,\n",
    "                \"reg\": self.reg,\n",
    "                \"sampler\": self.sampler\n",
    "            }\n",
    "        },\n",
    "        \"UserCF\": {\n",
    "            \"class\": UserCF,\n",
    "            \"params\": {\n",
    "                \"k_sim\": self.k_sim,\n",
    "                \"sim_type\": self.sim_type\n",
    "            }\n",
    "        },\n",
    "        \"ItemCF\": {\n",
    "            \"class\": ItemCF,\n",
    "            \"params\": {\n",
    "                \"k_sim\": self.k_sim,\n",
    "                \"sim_type\": self.sim_type\n",
    "            }\n",
    "        },\n",
    "        \"SVD\": {\n",
    "            \"class\": SVD,\n",
    "            \"params\": {\n",
    "                \"embed_size\": self.embed_size,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"lr\": self.lr,\n",
    "                \"reg\": self.reg\n",
    "            }\n",
    "        },\n",
    "        \"SVDpp\": {\n",
    "            \"class\": SVDpp,\n",
    "            \"params\": {\n",
    "                \"embed_size\": self.embed_size,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"lr\": self.lr,\n",
    "                \"reg\": self.reg,\n",
    "            }\n",
    "        },\n",
    "        \"ALS\": {\n",
    "            \"class\": ALS,\n",
    "            \"params\": {\n",
    "                \"embed_size\": self.embed_size,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"reg\": self.reg,\n",
    "                \"alpha\": 10,\n",
    "                \"use_cg\": True,\n",
    "                \"n_threads\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "        config = model_config.get(model_type)\n",
    "        if not config:\n",
    "            raise ValueError(f\"Unbekanntes Modell: {model_type}\")\n",
    "\n",
    "        self.model = config[\"class\"](**common_params, **config[\"params\"])\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "      if not self.model:\n",
    "          raise ValueError(\"Model not trained. Call set_model() first.\")\n",
    "\n",
    "     # Gemeinsame Parameter\n",
    "      common_params = {\n",
    "          \"verbose\": 2,\n",
    "          \"eval_data\": self.eval_data,\n",
    "          \"metrics\": [\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"]\n",
    "     }\n",
    "\n",
    "      # Modellspezifische Parameter\n",
    "      if isinstance(self.model, (UserCF, ItemCF)):\n",
    "          # Für Collaborative Filtering\n",
    "          fit_params = {\n",
    "              \"neg_sampling\": True,\n",
    "              \"verbose\": 1\n",
    "         }\n",
    "      else:\n",
    "          # Für Embedding-basierte Modelle: batch_size entfernen\n",
    "          fit_params = {\n",
    "              \"neg_sampling\": True,\n",
    "              \"shuffle\": True,\n",
    "             **common_params\n",
    "         }\n",
    "\n",
    "     # Training durchführen\n",
    "      self.model.fit(\n",
    "          self.train_data,\n",
    "          **fit_params\n",
    "      )\n",
    "\n",
    "    def load_and_preprocess(self, min_interactions):\n",
    "        \"\"\"Load and preprocess interaction data\"\"\"\n",
    "        # Download and load dataset\n",
    "        path = kagglehub.dataset_download(self.data_path)\n",
    "\n",
    "        # Load and combine interaction data\n",
    "        train = pd.read_csv(os.path.join(path, \"interactions_train.csv\"))\n",
    "        eval = pd.read_csv(os.path.join(path, \"interactions_validation.csv\"))\n",
    "        test = pd.read_csv(os.path.join(path, \"interactions_test.csv\"))\n",
    "\n",
    "        combined = pd.concat([train, eval, test], ignore_index=True)\n",
    "        combined = self._rename_and_filter_data(combined)\n",
    "\n",
    "        # Filter items\n",
    "        item_counts = combined[\"item\"].value_counts()\n",
    "        items_to_keep = item_counts[item_counts >= min_interactions].index\n",
    "        filtered = combined[combined[\"item\"].isin(items_to_keep)]\n",
    "\n",
    "        # Filter users\n",
    "        user_counts = filtered[\"user\"].value_counts()\n",
    "        users_to_keep = user_counts[user_counts >= min_interactions].index\n",
    "        self.data_filtered = filtered[filtered[\"user\"].isin(users_to_keep)]\n",
    "\n",
    "    def __prepare_data(self,model_type):\n",
    "      # Convert ratings to 0/1 for UserCF and ItemCF\n",
    "        if model_type in [\"UserCF\", \"ItemCF\"]:\n",
    "        # Binarize ratings: 0-2 → 0, 3-5 → 1\n",
    "          self.data_filtered['label'] = self.data_filtered['label'].apply(\n",
    "              lambda x: 0 if x <= 2 else 1\n",
    "          )\n",
    "      # Split data\n",
    "        self.train_data, self.eval_data, self.test_data = random_split(\n",
    "            self.data_filtered,\n",
    "            multi_ratios=[0.8, 0.1, 0.1]\n",
    "        )\n",
    "\n",
    "        # Build datasets\n",
    "        self.train_data, self.data_info = DatasetPure.build_trainset(self.train_data)\n",
    "        self.eval_data = DatasetPure.build_evalset(self.eval_data)\n",
    "        self.test_data = DatasetPure.build_testset(self.test_data)\n",
    "\n",
    "\n",
    "    def save_recommendations_as_csv(self,items_information,amount_of_recs, path):\n",
    "      df = self.get_recommendations(items_information,amount_of_recs)\n",
    "      df.to_csv(path, index=False)\n",
    "      return df\n",
    "\n",
    "    def get_recommendations(self, items_information, n_rec):\n",
    "        \"\"\"\n",
    "        Holt Empfehlungen für alle User in user_id_map und speichert die Ergebnisse in einem DataFrame.\n",
    "        \"\"\"\n",
    "        dfs = []\n",
    "        for user_identifier in self.user_id_map:\n",
    "            df = self.get_recommendation(user_identifier, n_rec, items_information)\n",
    "            dfs.append(df)\n",
    "        # Alle einzelnen DataFrames zusammenfügen\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        return final_df\n",
    "\n",
    "    def get_recommendation(self, user_identifier, n_rec, items_information):\n",
    "      \"\"\"Get recommendations for a user (UUID or numeric ID) und speichert alle Daten in einem DataFrame\"\"\"\n",
    "      if not self.model:\n",
    "          raise ValueError(\"Model not trained. Call train() first.\")\n",
    "\n",
    "      # UUID Lookup\n",
    "      if isinstance(user_identifier, str):\n",
    "          if user_identifier not in self.user_id_map:\n",
    "              raise ValueError(f\"User UUID '{user_identifier}' not found.\")\n",
    "          user_id = self.user_id_map[user_identifier]\n",
    "\n",
    "     # Empfehlungen abrufen\n",
    "      recommendations = self.model.recommend_user(\n",
    "          user=user_id,\n",
    "          n_rec=n_rec,\n",
    "          filter_consumed=True\n",
    "     )\n",
    "\n",
    "      # Liste für die Daten vorbereiten\n",
    "      records = []\n",
    "      for recipe in recommendations[user_id]:\n",
    "          # Item-Titel und Zutaten anhand der recipe_id abrufen\n",
    "          item_title, item_ingredients = self.__find_item_by_id(recipe, items_information)\n",
    "          # Datensatz zur Liste hinzufügen\n",
    "          records.append({\n",
    "              \"uuid\": user_identifier,\n",
    "              \"item_id\": recipe,\n",
    "             \"item_title\": item_title,\n",
    "              \"item_ingredients\": item_ingredients\n",
    "          })\n",
    "\n",
    "      # DataFrame aus der Liste erstellen\n",
    "      df = pd.DataFrame(records)\n",
    "      return df\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        return evaluate(\n",
    "            model=self.model,\n",
    "            data=self.test_data,\n",
    "            neg_sampling=True,\n",
    "            metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"]\n",
    "        )\n",
    "\n",
    "    def info(self, UUID):\n",
    "      \"\"\"Gibt einen DataFrame mit allen Interaktionen des angegebenen Benutzers (UUID) zurück.\"\"\"\n",
    "      # Überprüfen, ob Daten geladen wurden\n",
    "      if self.data_filtered is None or not isinstance(self.data_filtered, pd.DataFrame):\n",
    "          return pd.DataFrame(columns=[\"user\", \"item\", \"label\", \"name\"])\n",
    "\n",
    "      # Prüfen, ob die UUID vorhanden ist\n",
    "      if UUID not in self.user_id_map:\n",
    "          return pd.DataFrame(columns=[\"user\", \"item\", \"label\", \"name\"])\n",
    "\n",
    "      # Numerische Benutzer-ID abrufen\n",
    "      user_id = self.user_id_map[UUID]\n",
    "\n",
    "      # Interaktionen filtern\n",
    "      user_interactions = self.data_filtered[self.data_filtered['user'] == user_id].copy()\n",
    "\n",
    "      if user_interactions.empty:\n",
    "          return pd.DataFrame(columns=[\"user\", \"item\", \"label\", \"name\"])\n",
    "\n",
    "      # UUID statt numerischer ID setzen\n",
    "      user_interactions['user'] = UUID\n",
    "\n",
    "      # Rezeptnamen hinzufügen\n",
    "      merged = user_interactions.merge(self.name_df, left_on='item', right_on='id', how='left')\n",
    "      merged['name'] = merged['name'].fillna('Unknown Recipe')\n",
    "\n",
    "      # Ergebnis formatieren\n",
    "      result = merged[['user', 'item', 'label', 'name']]\n",
    "\n",
    "      return result\n",
    "\n",
    "    def save(self, storagepath):\n",
    "      \"\"\"Speichert Modell und Zustand\"\"\"\n",
    "      if not self.model:\n",
    "          raise ValueError(\"Modell nicht trainiert\")\n",
    "\n",
    "      os.makedirs(storagepath, exist_ok=True)\n",
    "\n",
    "      # 1. Modell mit LibreCos eigener Methode speichern\n",
    "      self.model.save(storagepath, model_name=\"BPR_model\")\n",
    "\n",
    "      # 2. User-Mapping als JSON\n",
    "      with open(os.path.join(storagepath, \"user_mapping.json\"), \"w\") as f:\n",
    "          json.dump(self.user_id_map, f)\n",
    "\n",
    "      # 3. Rezeptnamen-Daten\n",
    "      self.name_df.to_json(\n",
    "          os.path.join(storagepath, \"recipe_names.json\"),\n",
    "          orient=\"records\"\n",
    "      )\n",
    "\n",
    "      # 4. Gefilterte Daten\n",
    "      if self.data_filtered is not None:\n",
    "          self.data_filtered.to_parquet(\n",
    "             os.path.join(storagepath, \"filtered_data.parquet\")\n",
    "          )\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, storagepath):\n",
    "        \"\"\"Lädt gespeicherte Instanz\"\"\"\n",
    "        instance = cls.__new__(cls)\n",
    "        instance.data_path = None  # Nicht mehr relevant\n",
    "\n",
    "        # 1. Modell laden\n",
    "        instance.model = BPR.load(\n",
    "            path=storagepath,\n",
    "            model_name=\"BPR_model\",\n",
    "            data_info=None  # Wird automatisch geladen\n",
    "        )\n",
    "\n",
    "        # 2. DataInfo aus dem Modell holen\n",
    "        instance.data_info = instance.model.data_info\n",
    "\n",
    "        # 3. User-Mapping laden\n",
    "        with open(os.path.join(storagepath, \"user_mapping.json\"), \"r\") as f:\n",
    "            instance.user_id_map = json.load(f)\n",
    "\n",
    "        # 4. Rezeptnamen\n",
    "        instance.name_df = pd.read_json(\n",
    "            os.path.join(storagepath, \"recipe_names.json\"),\n",
    "            orient=\"records\"\n",
    "        )\n",
    "\n",
    "        # 5. Gefilterte Daten\n",
    "        instance.data_filtered = pd.read_parquet(\n",
    "            os.path.join(storagepath, \"filtered_data.parquet\")\n",
    "        )\n",
    "\n",
    "        return instance\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def _load_recipe_names(self):\n",
    "        \"\"\"Load recipe ID to name mapping\"\"\"\n",
    "        path = kagglehub.dataset_download(self.data_path)\n",
    "        raw_recipes_path = os.path.join(path, \"RAW_recipes.csv\")\n",
    "        self.name_df = pd.read_csv(raw_recipes_path)[[\"name\", \"id\"]]\n",
    "\n",
    "    def _rename_and_filter_data(self, interactions_data):\n",
    "      # Erzeuge explizite Kopie des DataFrames\n",
    "      df = interactions_data.copy()\n",
    "\n",
    "      # Spalten umbenennen (ohne inplace)\n",
    "      df = df.rename(columns={\n",
    "          \"user_id\": \"user\",\n",
    "          \"recipe_id\": \"item\",\n",
    "          \"rating\": \"label\"\n",
    "      })\n",
    "\n",
    "      # Spalten filtern\n",
    "      keep_cols = [\"user\", \"item\", \"label\"]\n",
    "      df = df[keep_cols]\n",
    "\n",
    "      # Typkonvertierung mit .loc\n",
    "      df.loc[:, \"label\"] = df[\"label\"].astype(int)\n",
    "      return df\n",
    "\n",
    "    def _get_recipe_name(self, recipe_id):\n",
    "        \"\"\"Helper to get recipe name from ID\"\"\"\n",
    "        name = self.name_df.loc[self.name_df['id'] == recipe_id, 'name']\n",
    "        return name.values[0] if not name.empty else \"Unknown Recipe\"\n",
    "\n",
    "    def import_ratings_csv(self, file_path):\n",
    "      \"\"\"Import ratings from CSV and map UUIDs to numeric IDs\"\"\"\n",
    "      try:\n",
    "          # Load CSV\n",
    "          df = pd.read_csv(file_path)\n",
    "          print(\"CSV erfolgreich geladen:\")\n",
    "          print(df.head())\n",
    "\n",
    "          # Check required columns\n",
    "          required = {\"uuid\", \"item_id\", \"rating\"}\n",
    "          if not required.issubset(df.columns):\n",
    "              missing = required - set(df.columns)\n",
    "              raise ValueError(f\"Fehlende Spalten: {missing}\")\n",
    "\n",
    "          # Process and map UUIDs\n",
    "          processed_df = self.__process_ratings(df)\n",
    "\n",
    "          # Add to data\n",
    "          self.data_filtered = pd.concat(\n",
    "              [self.data_filtered, processed_df],\n",
    "              ignore_index=True\n",
    "         )\n",
    "          print(f\"{len(processed_df)} neue Bewertungen hinzugefügt.\")\n",
    "\n",
    "      except FileNotFoundError:\n",
    "          print(f\"Datei {file_path} nicht gefunden.\")\n",
    "      except Exception as e:\n",
    "          print(f\"Fehler: {str(e)}\")\n",
    "\n",
    "    def __process_ratings(self, df):\n",
    "      \"\"\"Map UUIDs to numeric IDs\"\"\"\n",
    "      # Rename columns\n",
    "      df = df.rename(columns={\n",
    "          \"uuid\": \"user\",\n",
    "          \"item_id\": \"item\",\n",
    "          \"rating\": \"label\"\n",
    "      })\n",
    "\n",
    "      # Determine current max ID from user_id_map\n",
    "      current_max = max(self.user_id_map.values()) if self.user_id_map else 0\n",
    "\n",
    "      # Generate new IDs for unknown UUIDs\n",
    "      new_users = [uuid for uuid in df[\"user\"].unique() if uuid not in self.user_id_map]\n",
    "      num_new = len(new_users)\n",
    "\n",
    "      if num_new > 0:\n",
    "          new_ids = range(current_max + 1, current_max + num_new + 1)\n",
    "          self.user_id_map.update(zip(new_users, new_ids))\n",
    "\n",
    "      # Replace UUIDs with numeric IDs\n",
    "      df[\"user\"] = df[\"user\"].map(self.user_id_map)\n",
    "      return df\n",
    "\n",
    "\n",
    "    def __get_score(self,userid,itemid):\n",
    "     return self.model.predict(userid,itemid)\n",
    "\n",
    "    def __find_item_by_id(self,recipe_id, items_information):\n",
    "      df = items_information.loc[items_information[\"id\"] == recipe_id]\n",
    "      return df['name'].values[0], df['ingredients'].values[0]\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "def load_items_information():\n",
    "    path = kagglehub.dataset_download(\"shuyangli94/food-com-recipes-and-user-interactions\")\n",
    "\n",
    "    recipes_path = os.path.join(path, \"RAW_recipes.csv\")\n",
    "    recipes = pd.read_csv(recipes_path)\n",
    "\n",
    "    return recipes\n",
    "\n",
    "items_information = load_items_information()"
   ],
   "metadata": {
    "id": "QWp6kdNpY1zs",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "outputId": "3670a94f-13e3-4594-a013-e49013e03461",
    "ExecuteTime": {
     "end_time": "2025-03-28T12:12:22.122987Z",
     "start_time": "2025-03-28T12:12:15.682154Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ou5fJq46b9og",
    "outputId": "5a44f197-3cf7-48b9-a9d3-65df0c12acc0",
    "ExecuteTime": {
     "end_time": "2025-03-28T12:20:06.990062Z",
     "start_time": "2025-03-28T12:12:22.145132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import contextlib\n",
    "from config import RATINGS_FILE\n",
    "from config import RECOMMENDATIONS_FILE\n",
    "import io\n",
    "\n",
    "RECOMMENDER_TYPES = [\"SVD\", \"SVDpp\", \"ALS\", \"BPR\", \"UserCF\", \"ItemCF\"]\n",
    "\n",
    "for RECOMMENDER_TYPE in RECOMMENDER_TYPES:\n",
    "    print(f\"Training {RECOMMENDER_TYPE}...\")\n",
    "\n",
    "    with contextlib.redirect_stdout(io.StringIO()): # suppresses print statements\n",
    "        recommender = RecipeRecommender()\n",
    "        recommender.load_and_preprocess(min_interactions=20)\n",
    "        # Neue Nutzer per CSV importieren\n",
    "        recommender.import_ratings_csv(\"../\"+RATINGS_FILE)\n",
    "        recommender.set_model(RECOMMENDER_TYPE)\n",
    "        recommender.train()\n",
    "        eval = recommender.evaluate()\n",
    "\n",
    "    print(eval)\n",
    "    recommendations = recommender.save_recommendations_as_csv(\n",
    "        items_information,\n",
    "        20,\n",
    "        \"../\"+RECOMMENDATIONS_FILE+RECOMMENDER_TYPE.lower()+\".csv\"\n",
    "    )\n",
    "\n",
    "    print(f\"Recommendations for {RECOMMENDER_TYPE} saved to {RECOMMENDATIONS_FILE+RECOMMENDER_TYPE.lower()+'.csv'}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 826/826 [00:07<00:00, 112.75it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 131.11it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:02<00:00, 1097.78it/s]\n",
      "train: 100%|██████████| 826/826 [00:06<00:00, 121.49it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 84.88it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:02<00:00, 1047.42it/s]\n",
      "train: 100%|██████████| 826/826 [00:07<00:00, 116.05it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 50.45it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:01<00:00, 1488.58it/s]\n",
      "train: 100%|██████████| 826/826 [00:06<00:00, 120.98it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 69.41it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:02<00:00, 1078.01it/s]\n",
      "train: 100%|██████████| 826/826 [00:06<00:00, 121.22it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 76.68it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:02<00:00, 1136.92it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 122.24it/s]\n",
      "eval_listwise: 100%|██████████| 2398/2398 [00:01<00:00, 1412.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6897015339487874, 'roc_auc': 0.6565845309239092, 'precision': 0.01693077564637198, 'recall': 0.0356071656309301, 'ndcg': 0.07325318768743984}\n",
      "Recommendations for SVD saved to data/recommendations_svd.csv\n",
      "Training SVDpp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 826/826 [00:39<00:00, 21.04it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 95.54it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:13<00:00, 173.15it/s]\n",
      "train: 100%|██████████| 826/826 [00:37<00:00, 21.95it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 98.27it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:13<00:00, 175.31it/s]\n",
      "train: 100%|██████████| 826/826 [00:38<00:00, 21.50it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 101.23it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:10<00:00, 241.57it/s]\n",
      "train: 100%|██████████| 826/826 [00:41<00:00, 20.13it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 94.01it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:12<00:00, 190.42it/s]\n",
      "train: 100%|██████████| 826/826 [00:41<00:00, 19.80it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 132.79it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:14<00:00, 171.94it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 49.25it/s]\n",
      "eval_listwise: 100%|██████████| 2398/2398 [00:15<00:00, 154.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6837934940763822, 'roc_auc': 0.6672759640827447, 'precision': 0.017222685571309425, 'recall': 0.03692172173205574, 'ndcg': 0.07621165270280264}\n",
      "Recommendations for SVDpp saved to data/recommendations_svdpp.csv\n",
      "Training ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 122.25it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:02<00:00, 862.51it/s] \n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 137.09it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:03<00:00, 785.68it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 73.89it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:02<00:00, 994.40it/s] \n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 94.28it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:03<00:00, 705.92it/s] \n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 128.00it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:03<00:00, 705.52it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 110.61it/s]\n",
      "eval_listwise: 100%|██████████| 2398/2398 [00:02<00:00, 953.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6890487808294451, 'roc_auc': 0.5743027401388745, 'precision': 0.008465387823185988, 'recall': 0.018815749069898757, 'ndcg': 0.03853486094093693}\n",
      "Recommendations for ALS saved to data/recommendations_als.csv\n",
      "Training BPR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 518/518 [00:05<00:00, 94.03it/s] \n",
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 119.69it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:02<00:00, 1150.35it/s]\n",
      "train: 100%|██████████| 518/518 [00:05<00:00, 95.02it/s] \n",
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 95.66it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:02<00:00, 1076.84it/s]\n",
      "train: 100%|██████████| 518/518 [00:04<00:00, 108.84it/s]\n",
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 69.91it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:02<00:00, 1099.58it/s]\n",
      "train: 100%|██████████| 518/518 [00:05<00:00, 101.45it/s]\n",
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 96.47it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:02<00:00, 876.31it/s]\n",
      "train: 100%|██████████| 518/518 [00:04<00:00, 106.90it/s]\n",
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 109.93it/s]\n",
      "eval_listwise: 100%|██████████| 2422/2422 [00:01<00:00, 1234.13it/s]\n",
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 135.37it/s]\n",
      "eval_listwise: 100%|██████████| 2398/2398 [00:01<00:00, 1349.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6903956249494106, 'roc_auc': 0.631861379869741, 'precision': 0.014970809007506258, 'recall': 0.031106419505914903, 'ndcg': 0.06826200398370222}\n",
      "Recommendations for BPR saved to data/recommendations_bpr.csv\n",
      "Training UserCF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "top_k: 100%|██████████| 2511/2511 [00:01<00:00, 2343.67it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:01<00:00,  2.72it/s]\n",
      "eval_listwise: 100%|██████████| 2377/2377 [00:07<00:00, 297.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.952047078212844, 'roc_auc': 0.5812805783427327, 'precision': 0.01787968026924695, 'recall': 0.0359949285402003, 'ndcg': 0.0822709125469512}\n",
      "Recommendations for UserCF saved to data/recommendations_usercf.csv\n",
      "Training ItemCF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "top_k: 100%|██████████| 4501/4501 [00:00<00:00, 5304.64it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00,  7.80it/s]\n",
      "eval_listwise: 100%|██████████| 2377/2377 [00:07<00:00, 337.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.895334784087295, 'roc_auc': 0.48582276654435635, 'precision': 0.012326461926798487, 'recall': 0.026398516145144213, 'ndcg': 0.04887937038486083}\n",
      "Recommendations for ItemCF saved to data/recommendations_itemcf.csv\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T12:20:07.012894Z",
     "start_time": "2025-03-28T12:20:07.010856Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
