{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# !pip install pandas\n",
    "# !pip install --upgrade kagglehub\n",
    "# !pip install -U LibRecommender\n",
    "# !pip install keras==2.12.0 tensorflow==2.12.0\n",
    "#\n",
    "# !pip show LibRecommender"
   ],
   "metadata": {
    "id": "gS5ZkBCrWQOY",
    "ExecuteTime": {
     "end_time": "2025-03-28T13:40:22.478360Z",
     "start_time": "2025-03-28T13:40:22.472574Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kDA15rXbWIoB",
    "ExecuteTime": {
     "end_time": "2025-03-28T13:40:30.816316Z",
     "start_time": "2025-03-28T13:40:22.720787Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from libreco.data import random_split, DatasetPure\n",
    "from libreco.algorithms import BPR, UserCF, ItemCF, SVD, SVDpp, ALS\n",
    "from libreco.evaluation import evaluate\n",
    "import kagglehub\n",
    "import tensorflow as tf\n",
    "\n",
    "class RecipeRecommender:\n",
    "    def __init__(self, data_path=\"shuyangli94/food-com-recipes-and-user-interactions\"):\n",
    "        # Hyperparameter mit BPR als Referenz\n",
    "        self.embed_size = 256     # Für Embedding-basierte Modelle\n",
    "        self.n_epochs = 5         # Für trainierbare Modelle\n",
    "        self.lr = 5e-5            # Lernrate\n",
    "        self.reg = 5e-6           # Regularisierung\n",
    "        self.batch_size = 1024    # Batch-Größe\n",
    "        self.num_neg = 5          # Negative Samples\n",
    "        self.sampler = \"random\"   # Sampling-Methode\n",
    "        self.k_sim = 50           # Für CF-Modelle\n",
    "        self.sim_type = \"cosine\"  # Ähnlichkeitsmaß\n",
    "\n",
    "        # Gemeinsame Objekte\n",
    "        self.model = None\n",
    "        self.data_info = None\n",
    "        self.name_df = None\n",
    "        self.data_filtered = None\n",
    "        self.train_data = None\n",
    "        self.eval_data = None\n",
    "        self.test_data = None\n",
    "        self.user_id_map = {}\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self._load_recipe_names()\n",
    "\n",
    "\n",
    "    def set_model(self, model_type):\n",
    "        \"\"\"Zentrale Methode zur Modellauswahl mit einheitlichen Parametern\"\"\"\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        self.__prepare_data(model_type)  # Daten für alle Modelle vorbereiten\n",
    "\n",
    "        common_params = {\n",
    "            \"task\": \"ranking\",\n",
    "            \"data_info\": self.data_info\n",
    "        }\n",
    "\n",
    "        model_config = {\n",
    "           \"BPR\": {\n",
    "            \"class\": BPR,\n",
    "            \"params\": {\n",
    "                \"loss_type\": \"bpr\",\n",
    "                \"embed_size\": self.embed_size,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"lr\": self.lr,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"num_neg\": self.num_neg,\n",
    "                \"reg\": self.reg,\n",
    "                \"sampler\": self.sampler\n",
    "            }\n",
    "        },\n",
    "        \"UserCF\": {\n",
    "            \"class\": UserCF,\n",
    "            \"params\": {\n",
    "                \"k_sim\": self.k_sim,\n",
    "                \"sim_type\": self.sim_type\n",
    "            }\n",
    "        },\n",
    "        \"ItemCF\": {\n",
    "            \"class\": ItemCF,\n",
    "            \"params\": {\n",
    "                \"k_sim\": self.k_sim,\n",
    "                \"sim_type\": self.sim_type\n",
    "            }\n",
    "        },\n",
    "        \"SVD\": {\n",
    "            \"class\": SVD,\n",
    "            \"params\": {\n",
    "                \"embed_size\": self.embed_size,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"lr\": self.lr,\n",
    "                \"reg\": self.reg\n",
    "            }\n",
    "        },\n",
    "        \"SVDpp\": {\n",
    "            \"class\": SVDpp,\n",
    "            \"params\": {\n",
    "                \"embed_size\": self.embed_size,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"lr\": self.lr,\n",
    "                \"reg\": self.reg,\n",
    "            }\n",
    "        },\n",
    "        \"ALS\": {\n",
    "            \"class\": ALS,\n",
    "            \"params\": {\n",
    "                \"embed_size\": self.embed_size,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "                \"reg\": self.reg,\n",
    "                \"alpha\": 10,\n",
    "                \"use_cg\": True,\n",
    "                \"n_threads\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "        config = model_config.get(model_type)\n",
    "        if not config:\n",
    "            raise ValueError(f\"Unbekanntes Modell: {model_type}\")\n",
    "\n",
    "        self.model = config[\"class\"](**common_params, **config[\"params\"])\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "      if not self.model:\n",
    "          raise ValueError(\"Model not trained. Call set_model() first.\")\n",
    "\n",
    "     # Gemeinsame Parameter\n",
    "      common_params = {\n",
    "          \"verbose\": 2,\n",
    "          \"eval_data\": self.eval_data,\n",
    "          \"metrics\": [\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"]\n",
    "     }\n",
    "\n",
    "      # Modellspezifische Parameter\n",
    "      if isinstance(self.model, (UserCF, ItemCF)):\n",
    "          # Für Collaborative Filtering\n",
    "          fit_params = {\n",
    "              \"neg_sampling\": True,\n",
    "              \"verbose\": 1\n",
    "         }\n",
    "      else:\n",
    "          # Für Embedding-basierte Modelle: batch_size entfernen\n",
    "          fit_params = {\n",
    "              \"neg_sampling\": True,\n",
    "              \"shuffle\": True,\n",
    "             **common_params\n",
    "         }\n",
    "\n",
    "     # Training durchführen\n",
    "      self.model.fit(\n",
    "          self.train_data,\n",
    "          **fit_params\n",
    "      )\n",
    "\n",
    "    def load_and_preprocess(self, min_interactions):\n",
    "        \"\"\"Load and preprocess interaction data\"\"\"\n",
    "        # Download and load dataset\n",
    "        path = kagglehub.dataset_download(self.data_path)\n",
    "\n",
    "        # Load and combine interaction data\n",
    "        train = pd.read_csv(os.path.join(path, \"interactions_train.csv\"))\n",
    "        eval = pd.read_csv(os.path.join(path, \"interactions_validation.csv\"))\n",
    "        test = pd.read_csv(os.path.join(path, \"interactions_test.csv\"))\n",
    "\n",
    "        combined = pd.concat([train, eval, test], ignore_index=True)\n",
    "        combined = self._rename_and_filter_data(combined)\n",
    "\n",
    "        # Filter items\n",
    "        item_counts = combined[\"item\"].value_counts()\n",
    "        items_to_keep = item_counts[item_counts >= min_interactions].index\n",
    "        filtered = combined[combined[\"item\"].isin(items_to_keep)]\n",
    "\n",
    "        # Filter users\n",
    "        user_counts = filtered[\"user\"].value_counts()\n",
    "        users_to_keep = user_counts[user_counts >= min_interactions].index\n",
    "        self.data_filtered = filtered[filtered[\"user\"].isin(users_to_keep)]\n",
    "\n",
    "    def __prepare_data(self,model_type):\n",
    "      # Convert ratings to 0/1 for UserCF and ItemCF\n",
    "        if model_type in [\"UserCF\", \"ItemCF\"]:\n",
    "        # Binarize ratings: 0-2 → 0, 3-5 → 1\n",
    "          self.data_filtered['label'] = self.data_filtered['label'].apply(\n",
    "              lambda x: 0 if x <= 2 else 1\n",
    "          )\n",
    "      # Split data\n",
    "        self.train_data, self.eval_data, self.test_data = random_split(\n",
    "            self.data_filtered,\n",
    "            multi_ratios=[0.8, 0.1, 0.1]\n",
    "        )\n",
    "\n",
    "        # Build datasets\n",
    "        self.train_data, self.data_info = DatasetPure.build_trainset(self.train_data)\n",
    "        self.eval_data = DatasetPure.build_evalset(self.eval_data)\n",
    "        self.test_data = DatasetPure.build_testset(self.test_data)\n",
    "\n",
    "\n",
    "    def save_recommendations_as_csv(self,items_information,amount_of_recs, path):\n",
    "      df = self.get_recommendations(items_information,amount_of_recs)\n",
    "      df.to_csv(path, index=False)\n",
    "      return df\n",
    "\n",
    "    def get_recommendations(self, items_information, n_rec):\n",
    "        \"\"\"\n",
    "        Holt Empfehlungen für alle User in user_id_map und speichert die Ergebnisse in einem DataFrame.\n",
    "        \"\"\"\n",
    "        dfs = []\n",
    "        for user_identifier in self.user_id_map:\n",
    "            df = self.get_recommendation(user_identifier, n_rec, items_information)\n",
    "            dfs.append(df)\n",
    "        # Alle einzelnen DataFrames zusammenfügen\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        return final_df\n",
    "\n",
    "    def get_recommendation(self, user_identifier, n_rec, items_information):\n",
    "      \"\"\"Get recommendations for a user (UUID or numeric ID) und speichert alle Daten in einem DataFrame\"\"\"\n",
    "      if not self.model:\n",
    "          raise ValueError(\"Model not trained. Call train() first.\")\n",
    "\n",
    "      # UUID Lookup\n",
    "      if isinstance(user_identifier, str):\n",
    "          if user_identifier not in self.user_id_map:\n",
    "              raise ValueError(f\"User UUID '{user_identifier}' not found.\")\n",
    "          user_id = self.user_id_map[user_identifier]\n",
    "\n",
    "     # Empfehlungen abrufen\n",
    "      recommendations = self.model.recommend_user(\n",
    "          user=user_id,\n",
    "          n_rec=n_rec,\n",
    "          filter_consumed=True\n",
    "     )\n",
    "\n",
    "      # Liste für die Daten vorbereiten\n",
    "      records = []\n",
    "      for recipe in recommendations[user_id]:\n",
    "          # Item-Titel und Zutaten anhand der recipe_id abrufen\n",
    "          item_title, item_ingredients = self.__find_item_by_id(recipe, items_information)\n",
    "          # Datensatz zur Liste hinzufügen\n",
    "          records.append({\n",
    "              \"uuid\": user_identifier,\n",
    "              \"item_id\": recipe,\n",
    "             \"item_title\": item_title,\n",
    "              \"item_ingredients\": item_ingredients\n",
    "          })\n",
    "\n",
    "      # DataFrame aus der Liste erstellen\n",
    "      df = pd.DataFrame(records)\n",
    "      return df\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        return evaluate(\n",
    "            model=self.model,\n",
    "            data=self.test_data,\n",
    "            neg_sampling=True,\n",
    "            metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"]\n",
    "        )\n",
    "\n",
    "    def info(self, UUID):\n",
    "      \"\"\"Gibt einen DataFrame mit allen Interaktionen des angegebenen Benutzers (UUID) zurück.\"\"\"\n",
    "      # Überprüfen, ob Daten geladen wurden\n",
    "      if self.data_filtered is None or not isinstance(self.data_filtered, pd.DataFrame):\n",
    "          return pd.DataFrame(columns=[\"user\", \"item\", \"label\", \"name\"])\n",
    "\n",
    "      # Prüfen, ob die UUID vorhanden ist\n",
    "      if UUID not in self.user_id_map:\n",
    "          return pd.DataFrame(columns=[\"user\", \"item\", \"label\", \"name\"])\n",
    "\n",
    "      # Numerische Benutzer-ID abrufen\n",
    "      user_id = self.user_id_map[UUID]\n",
    "\n",
    "      # Interaktionen filtern\n",
    "      user_interactions = self.data_filtered[self.data_filtered['user'] == user_id].copy()\n",
    "\n",
    "      if user_interactions.empty:\n",
    "          return pd.DataFrame(columns=[\"user\", \"item\", \"label\", \"name\"])\n",
    "\n",
    "      # UUID statt numerischer ID setzen\n",
    "      user_interactions['user'] = UUID\n",
    "\n",
    "      # Rezeptnamen hinzufügen\n",
    "      merged = user_interactions.merge(self.name_df, left_on='item', right_on='id', how='left')\n",
    "      merged['name'] = merged['name'].fillna('Unknown Recipe')\n",
    "\n",
    "      # Ergebnis formatieren\n",
    "      result = merged[['user', 'item', 'label', 'name']]\n",
    "\n",
    "      return result\n",
    "\n",
    "    def save(self, storagepath):\n",
    "      \"\"\"Speichert Modell und Zustand\"\"\"\n",
    "      if not self.model:\n",
    "          raise ValueError(\"Modell nicht trainiert\")\n",
    "\n",
    "      os.makedirs(storagepath, exist_ok=True)\n",
    "\n",
    "      # 1. Modell mit LibreCos eigener Methode speichern\n",
    "      self.model.save(storagepath, model_name=\"BPR_model\")\n",
    "\n",
    "      # 2. User-Mapping als JSON\n",
    "      with open(os.path.join(storagepath, \"user_mapping.json\"), \"w\") as f:\n",
    "          json.dump(self.user_id_map, f)\n",
    "\n",
    "      # 3. Rezeptnamen-Daten\n",
    "      self.name_df.to_json(\n",
    "          os.path.join(storagepath, \"recipe_names.json\"),\n",
    "          orient=\"records\"\n",
    "      )\n",
    "\n",
    "      # 4. Gefilterte Daten\n",
    "      if self.data_filtered is not None:\n",
    "          self.data_filtered.to_parquet(\n",
    "             os.path.join(storagepath, \"filtered_data.parquet\")\n",
    "          )\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, storagepath):\n",
    "        \"\"\"Lädt gespeicherte Instanz\"\"\"\n",
    "        instance = cls.__new__(cls)\n",
    "        instance.data_path = None  # Nicht mehr relevant\n",
    "\n",
    "        # 1. Modell laden\n",
    "        instance.model = BPR.load(\n",
    "            path=storagepath,\n",
    "            model_name=\"BPR_model\",\n",
    "            data_info=None  # Wird automatisch geladen\n",
    "        )\n",
    "\n",
    "        # 2. DataInfo aus dem Modell holen\n",
    "        instance.data_info = instance.model.data_info\n",
    "\n",
    "        # 3. User-Mapping laden\n",
    "        with open(os.path.join(storagepath, \"user_mapping.json\"), \"r\") as f:\n",
    "            instance.user_id_map = json.load(f)\n",
    "\n",
    "        # 4. Rezeptnamen\n",
    "        instance.name_df = pd.read_json(\n",
    "            os.path.join(storagepath, \"recipe_names.json\"),\n",
    "            orient=\"records\"\n",
    "        )\n",
    "\n",
    "        # 5. Gefilterte Daten\n",
    "        instance.data_filtered = pd.read_parquet(\n",
    "            os.path.join(storagepath, \"filtered_data.parquet\")\n",
    "        )\n",
    "\n",
    "        return instance\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def _load_recipe_names(self):\n",
    "        \"\"\"Load recipe ID to name mapping\"\"\"\n",
    "        path = kagglehub.dataset_download(self.data_path)\n",
    "        raw_recipes_path = os.path.join(path, \"RAW_recipes.csv\")\n",
    "        self.name_df = pd.read_csv(raw_recipes_path)[[\"name\", \"id\"]]\n",
    "\n",
    "    def _rename_and_filter_data(self, interactions_data):\n",
    "      # Erzeuge explizite Kopie des DataFrames\n",
    "      df = interactions_data.copy()\n",
    "\n",
    "      # Spalten umbenennen (ohne inplace)\n",
    "      df = df.rename(columns={\n",
    "          \"user_id\": \"user\",\n",
    "          \"recipe_id\": \"item\",\n",
    "          \"rating\": \"label\"\n",
    "      })\n",
    "\n",
    "      # Spalten filtern\n",
    "      keep_cols = [\"user\", \"item\", \"label\"]\n",
    "      df = df[keep_cols]\n",
    "\n",
    "      # Typkonvertierung mit .loc\n",
    "      df.loc[:, \"label\"] = df[\"label\"].astype(int)\n",
    "      return df\n",
    "\n",
    "    def _get_recipe_name(self, recipe_id):\n",
    "        \"\"\"Helper to get recipe name from ID\"\"\"\n",
    "        name = self.name_df.loc[self.name_df['id'] == recipe_id, 'name']\n",
    "        return name.values[0] if not name.empty else \"Unknown Recipe\"\n",
    "\n",
    "    def import_ratings_csv(self, file_path):\n",
    "      \"\"\"Import ratings from CSV and map UUIDs to numeric IDs\"\"\"\n",
    "      try:\n",
    "          # Load CSV\n",
    "          df = pd.read_csv(file_path)\n",
    "          print(\"CSV erfolgreich geladen:\")\n",
    "          print(df.head())\n",
    "\n",
    "          # Check required columns\n",
    "          required = {\"uuid\", \"item_id\", \"rating\"}\n",
    "          if not required.issubset(df.columns):\n",
    "              missing = required - set(df.columns)\n",
    "              raise ValueError(f\"Fehlende Spalten: {missing}\")\n",
    "\n",
    "          # Process and map UUIDs\n",
    "          processed_df = self.__process_ratings(df)\n",
    "\n",
    "          # Add to data\n",
    "          self.data_filtered = pd.concat(\n",
    "              [self.data_filtered, processed_df],\n",
    "              ignore_index=True\n",
    "         )\n",
    "          print(f\"{len(processed_df)} neue Bewertungen hinzugefügt.\")\n",
    "\n",
    "      except FileNotFoundError:\n",
    "          print(f\"Datei {file_path} nicht gefunden.\")\n",
    "      except Exception as e:\n",
    "          print(f\"Fehler: {str(e)}\")\n",
    "\n",
    "    def __process_ratings(self, df):\n",
    "      \"\"\"Map UUIDs to numeric IDs\"\"\"\n",
    "      # Rename columns\n",
    "      df = df.rename(columns={\n",
    "          \"uuid\": \"user\",\n",
    "          \"item_id\": \"item\",\n",
    "          \"rating\": \"label\"\n",
    "      })\n",
    "\n",
    "      # Convert from range [-2,2] to [1,5]\n",
    "      df[\"label\"] = df[\"label\"] + 3\n",
    "\n",
    "      # Determine current max ID from user_id_map\n",
    "      current_max = max(self.user_id_map.values()) if self.user_id_map else 0\n",
    "\n",
    "      # Generate new IDs for unknown UUIDs\n",
    "      new_users = [uuid for uuid in df[\"user\"].unique() if uuid not in self.user_id_map]\n",
    "      num_new = len(new_users)\n",
    "\n",
    "      if num_new > 0:\n",
    "          new_ids = range(current_max + 1, current_max + num_new + 1)\n",
    "          self.user_id_map.update(zip(new_users, new_ids))\n",
    "\n",
    "      # Replace UUIDs with numeric IDs\n",
    "      df[\"user\"] = df[\"user\"].map(self.user_id_map)\n",
    "      return df\n",
    "\n",
    "\n",
    "    def __get_score(self,userid,itemid):\n",
    "     return self.model.predict(userid,itemid)\n",
    "\n",
    "    def __find_item_by_id(self,recipe_id, items_information):\n",
    "      df = items_information.loc[items_information[\"id\"] == recipe_id]\n",
    "      return df['name'].values[0], df['ingredients'].values[0]\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 14:40:24.583218: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-28 14:40:24.587408: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-28 14:40:24.666854: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-28 14:40:24.668238: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-28 14:40:25.849167: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mw/anaconda3/envs/repr-u1-10/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mw/anaconda3/envs/repr-u1-10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "def load_items_information():\n",
    "    path = kagglehub.dataset_download(\"shuyangli94/food-com-recipes-and-user-interactions\")\n",
    "\n",
    "    recipes_path = os.path.join(path, \"RAW_recipes.csv\")\n",
    "    recipes = pd.read_csv(recipes_path)\n",
    "\n",
    "    return recipes\n",
    "\n",
    "items_information = load_items_information()"
   ],
   "metadata": {
    "id": "QWp6kdNpY1zs",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "outputId": "3670a94f-13e3-4594-a013-e49013e03461",
    "ExecuteTime": {
     "end_time": "2025-03-28T13:40:38.224429Z",
     "start_time": "2025-03-28T13:40:30.822992Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ou5fJq46b9og",
    "outputId": "5a44f197-3cf7-48b9-a9d3-65df0c12acc0",
    "ExecuteTime": {
     "end_time": "2025-03-28T13:47:56.918516Z",
     "start_time": "2025-03-28T13:41:43.184508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import contextlib\n",
    "from config import RATINGS_FILE\n",
    "from config import RECOMMENDATIONS_FILE\n",
    "import io\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "RECOMMENDER_TYPES = [\"SVD\", \"SVDpp\", \"ALS\", \"BPR\", \"UserCF\", \"ItemCF\"]\n",
    "\n",
    "for RECOMMENDER_TYPE in RECOMMENDER_TYPES:\n",
    "    print(f\"Training {RECOMMENDER_TYPE}...\")\n",
    "\n",
    "    with contextlib.redirect_stdout(io.StringIO()): # suppresses print statements\n",
    "        recommender = RecipeRecommender()\n",
    "        recommender.load_and_preprocess(min_interactions=20)\n",
    "        # Neue Nutzer per CSV importieren\n",
    "        recommender.import_ratings_csv(\"../\"+RATINGS_FILE)\n",
    "        recommender.set_model(RECOMMENDER_TYPE)\n",
    "        recommender.train()\n",
    "        eval = recommender.evaluate()\n",
    "\n",
    "    print(eval)\n",
    "    recommendations = recommender.save_recommendations_as_csv(\n",
    "        items_information,\n",
    "        20,\n",
    "        \"../\"+RECOMMENDATIONS_FILE+RECOMMENDER_TYPE.lower()+\".csv\"\n",
    "    )\n",
    "\n",
    "    print(f\"Recommendations for {RECOMMENDER_TYPE} saved to {RECOMMENDATIONS_FILE+RECOMMENDER_TYPE.lower()+'.csv'}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 826/826 [00:07<00:00, 111.20it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 122.25it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:02<00:00, 861.40it/s] \n",
      "train: 100%|██████████| 826/826 [00:07<00:00, 113.34it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 109.61it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:01<00:00, 1226.17it/s]\n",
      "train: 100%|██████████| 826/826 [00:07<00:00, 116.47it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 61.09it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:02<00:00, 1073.37it/s]\n",
      "train: 100%|██████████| 826/826 [00:06<00:00, 126.53it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 62.23it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:02<00:00, 915.60it/s] \n",
      "train: 100%|██████████| 826/826 [00:07<00:00, 114.51it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 69.46it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:02<00:00, 1092.16it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 127.58it/s]\n",
      "eval_listwise: 100%|██████████| 2401/2401 [00:02<00:00, 1163.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6897015658138147, 'roc_auc': 0.6565810005799055, 'precision': 0.01699291961682632, 'recall': 0.03549325969025561, 'ndcg': 0.07396739645880177}\n",
      "Recommendations for SVD saved to data/recommendations_svd.csv\n",
      "Training SVDpp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 826/826 [00:31<00:00, 26.56it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 82.03it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:02<00:00, 1182.08it/s]\n",
      "train: 100%|██████████| 826/826 [00:30<00:00, 26.75it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 78.95it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:02<00:00, 1188.93it/s]\n",
      "train: 100%|██████████| 826/826 [00:31<00:00, 25.96it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 146.85it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:02<00:00, 928.31it/s]\n",
      "train: 100%|██████████| 826/826 [00:31<00:00, 25.94it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 82.32it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:01<00:00, 1488.19it/s]\n",
      "train: 100%|██████████| 826/826 [00:28<00:00, 29.25it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 129.73it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:01<00:00, 1357.71it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 127.72it/s]\n",
      "eval_listwise: 100%|██████████| 2401/2401 [00:01<00:00, 1463.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6838146272403518, 'roc_auc': 0.6672400956729534, 'precision': 0.0172844648063307, 'recall': 0.03680617328063432, 'ndcg': 0.07711007809157268}\n",
      "Recommendations for SVDpp saved to data/recommendations_svdpp.csv\n",
      "Training ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 98.82it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:01<00:00, 1308.66it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 126.90it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:02<00:00, 1085.24it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 141.06it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:01<00:00, 1688.61it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 117.59it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:01<00:00, 1224.14it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 116.22it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:02<00:00, 1152.99it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00, 130.96it/s]\n",
      "eval_listwise: 100%|██████████| 2401/2401 [00:01<00:00, 1435.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6882760365096353, 'roc_auc': 0.5772709535753398, 'precision': 0.011078717201166181, 'recall': 0.02681027911306318, 'ndcg': 0.0488404225760915}\n",
      "Recommendations for ALS saved to data/recommendations_als.csv\n",
      "Training BPR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 518/518 [00:04<00:00, 113.13it/s]\n",
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 156.42it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:01<00:00, 1505.89it/s]\n",
      "train: 100%|██████████| 518/518 [00:04<00:00, 117.78it/s]\n",
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 129.37it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:01<00:00, 1464.08it/s]\n",
      "train: 100%|██████████| 518/518 [00:04<00:00, 119.86it/s]\n",
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 104.58it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:02<00:00, 1144.63it/s]\n",
      "train: 100%|██████████| 518/518 [00:04<00:00, 117.60it/s]\n",
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 118.69it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:01<00:00, 1710.60it/s]\n",
      "train: 100%|██████████| 518/518 [00:04<00:00, 114.29it/s]\n",
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 118.31it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:01<00:00, 1303.73it/s]\n",
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 101.75it/s]\n",
      "eval_listwise: 100%|██████████| 2401/2401 [00:01<00:00, 1567.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6903950809418427, 'roc_auc': 0.6318738017479029, 'precision': 0.015285297792586423, 'recall': 0.03154751142339229, 'ndcg': 0.06928626284982899}\n",
      "Recommendations for BPR saved to data/recommendations_bpr.csv\n",
      "Training UserCF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "top_k: 100%|██████████| 2511/2511 [00:01<00:00, 2106.06it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:00<00:00,  4.68it/s]\n",
      "eval_listwise: 100%|██████████| 2396/2396 [00:20<00:00, 118.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.355674405003917, 'roc_auc': 0.5812624763676306, 'precision': 0.019198664440734557, 'recall': 0.04360727131182514, 'ndcg': 0.0877945792068934}\n",
      "Recommendations for UserCF saved to data/recommendations_usercf.csv\n",
      "Training ItemCF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "top_k: 100%|██████████| 4501/4501 [00:03<00:00, 1434.08it/s]\n",
      "eval_pointwise: 100%|██████████| 4/4 [00:01<00:00,  2.93it/s]\n",
      "eval_listwise: 100%|██████████| 2396/2396 [00:19<00:00, 123.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.872964075712602, 'roc_auc': 0.4873305704457747, 'precision': 0.013564273789649415, 'recall': 0.033787987858411445, 'ndcg': 0.05477304121673165}\n",
      "Recommendations for ItemCF saved to data/recommendations_itemcf.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T13:41:09.291285735Z",
     "start_time": "2025-03-28T12:54:39.822374Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
