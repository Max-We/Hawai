{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install --upgrade kagglehub\n",
        "!pip install -U LibRecommender\n",
        "!pip install keras==2.12.0 tensorflow==2.12.0\n",
        "\n",
        "!pip show LibRecommender"
      ],
      "metadata": {
        "id": "gS5ZkBCrWQOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r90DxNMZ090y",
        "outputId": "62a82c60-b9db-443b-a487-0d7d8b9d6b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "kDA15rXbWIoB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from libreco.data import random_split, DatasetPure\n",
        "from libreco.algorithms import BPR, UserCF, ItemCF, SVD, SVDpp, ALS\n",
        "from libreco.evaluation import evaluate\n",
        "import kagglehub\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class RecipeRecommender:\n",
        "    def __init__(self, data_path=\"shuyangli94/food-com-recipes-and-user-interactions\"):\n",
        "        # Hyperparameter mit BPR als Referenz\n",
        "        self.embed_size = 64     # Für Embedding-basierte Modelle\n",
        "        self.n_epochs = 8         # Für trainierbare Modelle\n",
        "        self.lr = 5e-5            # Lernrate\n",
        "        self.reg = 5e-6           # Regularisierung\n",
        "        self.batch_size = 1024    # Batch-Größe\n",
        "        self.num_neg = 10          # Negative Samples\n",
        "        self.sampler = \"random\"   # Sampling-Methode\n",
        "        self.k_sim = 50           # Für CF-Modelle\n",
        "        self.sim_type = \"cosine\"  # Ähnlichkeitsmaß\n",
        "\n",
        "        # Gemeinsame Objekte\n",
        "        self.model = None\n",
        "        self.data_info = None\n",
        "        self.name_df = None\n",
        "        self.data_filtered = None\n",
        "        self.train_data = None\n",
        "        self.eval_data = None\n",
        "        self.test_data = None\n",
        "        self.user_id_map = {}\n",
        "        self.item_popularity = None\n",
        "\n",
        "        self.data_path = data_path\n",
        "        self._load_recipe_names()\n",
        "\n",
        "\n",
        "    def set_model(self, model_type):\n",
        "        \"\"\"Zentrale Methode zur Modellauswahl mit einheitlichen Parametern\"\"\"\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "        self.__prepare_data(model_type)  # Daten für alle Modelle vorbereiten\n",
        "\n",
        "        common_params = {\n",
        "            \"task\": \"ranking\",\n",
        "            \"data_info\": self.data_info\n",
        "        }\n",
        "\n",
        "        model_config = {\n",
        "           \"BPR\": {\n",
        "            \"class\": BPR,\n",
        "            \"params\": {\n",
        "                \"loss_type\": \"bpr\",\n",
        "                \"embed_size\": self.embed_size,\n",
        "                \"n_epochs\": self.n_epochs,\n",
        "                \"lr\": self.lr,\n",
        "                \"batch_size\": self.batch_size,\n",
        "                \"num_neg\": self.num_neg,\n",
        "                \"reg\": self.reg,\n",
        "                \"sampler\": self.sampler\n",
        "            }\n",
        "        },\n",
        "        \"UserCF\": {\n",
        "            \"class\": UserCF,\n",
        "            \"params\": {\n",
        "                \"k_sim\": self.k_sim,\n",
        "                \"sim_type\": self.sim_type\n",
        "            }\n",
        "        },\n",
        "        \"ItemCF\": {\n",
        "            \"class\": ItemCF,\n",
        "            \"params\": {\n",
        "                \"k_sim\": self.k_sim,\n",
        "                \"sim_type\": self.sim_type\n",
        "            }\n",
        "        },\n",
        "        \"SVD\": {\n",
        "            \"class\": SVD,\n",
        "            \"params\": {\n",
        "                \"embed_size\": self.embed_size,\n",
        "                \"n_epochs\": self.n_epochs,\n",
        "                \"lr\": self.lr,\n",
        "                \"reg\": self.reg\n",
        "            }\n",
        "        },\n",
        "        \"SVDpp\": {\n",
        "            \"class\": SVDpp,\n",
        "            \"params\": {\n",
        "                \"embed_size\": self.embed_size,\n",
        "                \"n_epochs\": self.n_epochs,\n",
        "                \"lr\": self.lr,\n",
        "                \"reg\": self.reg,\n",
        "            }\n",
        "        },\n",
        "        \"ALS\": {\n",
        "            \"class\": ALS,\n",
        "            \"params\": {\n",
        "                \"embed_size\": self.embed_size,\n",
        "                \"n_epochs\": self.n_epochs,\n",
        "                \"reg\": self.reg,\n",
        "                \"alpha\": 10,\n",
        "                \"use_cg\": True,\n",
        "                \"n_threads\": 1\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "        config = model_config.get(model_type)\n",
        "        if not config:\n",
        "            raise ValueError(f\"Unbekanntes Modell: {model_type}\")\n",
        "\n",
        "        self.model = config[\"class\"](**common_params, **config[\"params\"])\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "      if not self.model:\n",
        "          raise ValueError(\"Model not trained. Call set_model() first.\")\n",
        "\n",
        "     # Gemeinsame Parameter\n",
        "      common_params = {\n",
        "          \"verbose\": 2,\n",
        "          \"eval_data\": self.eval_data,\n",
        "          \"metrics\": [\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"]\n",
        "     }\n",
        "\n",
        "      # Modellspezifische Parameter\n",
        "      if isinstance(self.model, (UserCF, ItemCF)):\n",
        "          # Für Collaborative Filtering\n",
        "          fit_params = {\n",
        "              \"neg_sampling\": True,\n",
        "              \"verbose\": 1\n",
        "         }\n",
        "      else:\n",
        "          # Für Embedding-basierte Modelle: batch_size entfernen\n",
        "          fit_params = {\n",
        "              \"neg_sampling\": True,\n",
        "              \"shuffle\": True,\n",
        "             **common_params\n",
        "         }\n",
        "\n",
        "     # Training durchführen\n",
        "      self.model.fit(\n",
        "          self.train_data,\n",
        "          **fit_params\n",
        "      )\n",
        "      self.item_popularity = self.data_filtered[\"item\"].value_counts().to_dict()\n",
        "\n",
        "\n",
        "    def load_and_preprocess(self, min_interactions):\n",
        "        \"\"\"Load and preprocess interaction data\"\"\"\n",
        "        # Download and load dataset\n",
        "        path = kagglehub.dataset_download(self.data_path)\n",
        "\n",
        "        # Load and combine interaction data\n",
        "        train = pd.read_csv(os.path.join(path, \"interactions_train.csv\"))\n",
        "        eval = pd.read_csv(os.path.join(path, \"interactions_validation.csv\"))\n",
        "        test = pd.read_csv(os.path.join(path, \"interactions_test.csv\"))\n",
        "\n",
        "        combined = pd.concat([train, eval, test], ignore_index=True)\n",
        "        combined = self._rename_and_filter_data(combined)\n",
        "\n",
        "        # Filter items\n",
        "        item_counts = combined[\"item\"].value_counts()\n",
        "        items_to_keep = item_counts[item_counts >= min_interactions].index\n",
        "        filtered = combined[combined[\"item\"].isin(items_to_keep)]\n",
        "\n",
        "        # Filter users\n",
        "        user_counts = filtered[\"user\"].value_counts()\n",
        "        users_to_keep = user_counts[user_counts >= min_interactions].index\n",
        "        self.data_filtered = filtered[filtered[\"user\"].isin(users_to_keep)]\n",
        "\n",
        "        print(\"Wie viele interactions gibt es?\" + str(len(self.data_filtered)))\n",
        "\n",
        "    def __prepare_data(self,model_type):\n",
        "      # Convert ratings to 0/1 for UserCF and ItemCF\n",
        "        if model_type in [\"UserCF\", \"ItemCF\"]:\n",
        "        # Binarize ratings: 0-2 → 0, 3-5 → 1\n",
        "          self.data_filtered['label'] = self.data_filtered['label'].apply(\n",
        "              lambda x: 0 if x <= 2 else 1\n",
        "          )\n",
        "      # Split data\n",
        "        self.train_data, self.eval_data, self.test_data = random_split(\n",
        "            self.data_filtered,\n",
        "            multi_ratios=[0.8, 0.1, 0.1]\n",
        "        )\n",
        "\n",
        "        # Build datasets\n",
        "        self.train_data, self.data_info = DatasetPure.build_trainset(self.train_data)\n",
        "        self.eval_data = DatasetPure.build_evalset(self.eval_data)\n",
        "        self.test_data = DatasetPure.build_testset(self.test_data)\n",
        "\n",
        "\n",
        "    def save_recommendations_as_csv(self,items_information,amount_of_recs, path):\n",
        "      df = self.get_recommendations(items_information,amount_of_recs)\n",
        "      df.to_csv(path, index=False)\n",
        "      return df\n",
        "\n",
        "    def get_recommendations(self, items_information, n_rec):\n",
        "        \"\"\"\n",
        "        Holt Empfehlungen für alle User in user_id_map und speichert die Ergebnisse in einem DataFrame.\n",
        "        \"\"\"\n",
        "        dfs = []\n",
        "        for user_identifier in self.user_id_map:\n",
        "            df = self.get_recommendation(user_identifier, n_rec, items_information)\n",
        "            dfs.append(df)\n",
        "        # Alle einzelnen DataFrames zusammenfügen\n",
        "        final_df = pd.concat(dfs, ignore_index=True)\n",
        "        return final_df\n",
        "\n",
        "    def get_recommendation(self, user_identifier, n_rec, items_information):\n",
        "        \"\"\"Get personalized recommendations with popularity balancing\"\"\"\n",
        "        try:\n",
        "            # Modell-Check\n",
        "            if not self.model:\n",
        "                raise ValueError(\"Model not trained. Call train() first.\")\n",
        "\n",
        "            # User-ID Mapping\n",
        "            user_id = user_identifier\n",
        "            if isinstance(user_identifier, str):\n",
        "                if user_identifier not in self.user_id_map:\n",
        "                    raise ValueError(f\"User UUID '{user_identifier}' not found\")\n",
        "                user_id = self.user_id_map[user_identifier]\n",
        "\n",
        "            # Initiale Kandidaten mit 5x Überabtastung\n",
        "            recommendations = self.model.recommend_user(\n",
        "                user=user_id, n_rec=n_rec * 5, filter_consumed=True\n",
        "            )\n",
        "            # Convert each item to a Python integer to avoid numpy types\n",
        "            candidate_items = [int(item) for item in recommendations.get(user_id, [])]\n",
        "\n",
        "            # Check if candidate_items is empty (explicit length check)\n",
        "            if len(candidate_items) == 0:\n",
        "                print(f\"No candidates for user {user_id}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            # Berechnung der Long-Tail-Präferenz\n",
        "            user_interactions = self.data_filtered[self.data_filtered['user'] == user_id]['item']\n",
        "            user_popularity = [self.item_popularity.get(item, 0) for item in user_interactions]\n",
        "\n",
        "            if user_popularity:\n",
        "                median_pop = np.median(user_popularity)\n",
        "                long_tail_ratio = sum(pop < median_pop for pop in user_popularity) / len(user_popularity)\n",
        "                max_popularity = max(self.item_popularity.values())\n",
        "            else:\n",
        "                # Fallback für neue Nutzer\n",
        "                long_tail_ratio = 0.5\n",
        "                max_popularity = max(self.item_popularity.values()) if self.item_popularity else 1\n",
        "\n",
        "            # Klassifizierung in Short-Head und Long-Tail\n",
        "            sorted_popularity = sorted(self.item_popularity.values(), reverse=True)\n",
        "            threshold_idx = int(0.2 * len(sorted_popularity))\n",
        "            short_head_threshold = sorted_popularity[threshold_idx] if sorted_popularity else 0\n",
        "\n",
        "            short_head = {item for item in candidate_items\n",
        "                        if self.item_popularity.get(item, 0) >= short_head_threshold}\n",
        "            long_tail = set(candidate_items) - short_head\n",
        "\n",
        "            # Iteratives xQuAD-basiertes Re-Ranking\n",
        "            lambda_param = 0.6  # Trade-off-Parameter\n",
        "            selected = []\n",
        "            remaining = candidate_items.copy()\n",
        "\n",
        "            while len(selected) < n_rec and remaining:\n",
        "                best_score = -1\n",
        "                best_item = None\n",
        "\n",
        "                for idx, item in enumerate(remaining):\n",
        "                    # Relevanz basierend auf ursprünglichem Ranking\n",
        "                    relevance = 1 - (idx / len(remaining))\n",
        "\n",
        "                    # Diversitätsbonus für Long-Tail-Items\n",
        "                    diversity_bonus = 1.0\n",
        "                    if item in long_tail:\n",
        "                        diversity_bonus += lambda_param * long_tail_ratio\n",
        "\n",
        "                    # Popularitätsadjustierung\n",
        "                    popularity = self.item_popularity.get(item, 0)\n",
        "                    popularity_score = (1 - popularity / max_popularity)\n",
        "\n",
        "                    total_score = (0.7 * relevance) + (0.3 * diversity_bonus * popularity_score)\n",
        "\n",
        "                    if total_score > best_score:\n",
        "                        best_score = total_score\n",
        "                        best_item = item\n",
        "\n",
        "                if best_item:\n",
        "                    selected.append(best_item)\n",
        "                    remaining.remove(best_item)\n",
        "\n",
        "            # Metadaten-Anreicherung mit verbesserter Fehlerbehandlung\n",
        "            records = []\n",
        "            for item_id in selected[:n_rec]:\n",
        "                try:\n",
        "                    title, ingredients = self.__find_item_by_id(item_id, items_information)\n",
        "                    records.append({\n",
        "                        \"uuid\": user_identifier,\n",
        "                        \"item_id\": item_id,\n",
        "                        \"item_title\": title or \"Unknown\",\n",
        "                        \"item_ingredients\": ingredients or []\n",
        "                    })\n",
        "                except (KeyError, IndexError) as e:\n",
        "                    print(f\"Metadata error for item {item_id}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            # Berechnung der Evaluierungsmetriken\n",
        "            if records:\n",
        "                arp = sum(item['popularity'] for item in records) / len(records)\n",
        "                aplt = sum(1 for item in records if item['popularity'] < short_head_threshold) / len(records)\n",
        "                print(f\"Recommender stats - ARP: {arp:.2f}, APLT: {aplt:.2f}\")\n",
        "\n",
        "            return pd.DataFrame(records).drop_duplicates().head(n_rec)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Recommendation error for user {user_id}: {str(e)}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        return evaluate(\n",
        "            model=self.model,\n",
        "            data=self.test_data,\n",
        "            neg_sampling=True,\n",
        "            metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"]\n",
        "        )\n",
        "\n",
        "    def info(self, UUID):\n",
        "      \"\"\"Gibt einen DataFrame mit allen Interaktionen des angegebenen Benutzers (UUID) zurück.\"\"\"\n",
        "      # Überprüfen, ob Daten geladen wurden\n",
        "      if self.data_filtered is None or not isinstance(self.data_filtered, pd.DataFrame):\n",
        "          return pd.DataFrame(columns=[\"user\", \"item\", \"label\", \"name\"])\n",
        "\n",
        "      # Prüfen, ob die UUID vorhanden ist\n",
        "      if UUID not in self.user_id_map:\n",
        "          return pd.DataFrame(columns=[\"user\", \"item\", \"label\", \"name\"])\n",
        "\n",
        "      # Numerische Benutzer-ID abrufen\n",
        "      user_id = self.user_id_map[UUID]\n",
        "\n",
        "      # Interaktionen filtern\n",
        "      user_interactions = self.data_filtered[self.data_filtered['user'] == user_id].copy()\n",
        "\n",
        "      if user_interactions.empty:\n",
        "          return pd.DataFrame(columns=[\"user\", \"item\", \"label\", \"name\"])\n",
        "\n",
        "      # UUID statt numerischer ID setzen\n",
        "      user_interactions['user'] = UUID\n",
        "\n",
        "      # Rezeptnamen hinzufügen\n",
        "      merged = user_interactions.merge(self.name_df, left_on='item', right_on='id', how='left')\n",
        "      merged['name'] = merged['name'].fillna('Unknown Recipe')\n",
        "\n",
        "      # Ergebnis formatieren\n",
        "      result = merged[['user', 'item', 'label', 'name']]\n",
        "\n",
        "      return result\n",
        "\n",
        "    def save(self, storagepath):\n",
        "      \"\"\"Speichert Modell und Zustand\"\"\"\n",
        "      if not self.model:\n",
        "          raise ValueError(\"Modell nicht trainiert\")\n",
        "\n",
        "      os.makedirs(storagepath, exist_ok=True)\n",
        "\n",
        "      # 1. Modell mit LibreCos eigener Methode speichern\n",
        "      self.model.save(storagepath, model_name=\"BPR_model\")\n",
        "\n",
        "      # 2. User-Mapping als JSON\n",
        "      with open(os.path.join(storagepath, \"user_mapping.json\"), \"w\") as f:\n",
        "          json.dump(self.user_id_map, f)\n",
        "\n",
        "      # 3. Rezeptnamen-Daten\n",
        "      self.name_df.to_json(\n",
        "          os.path.join(storagepath, \"recipe_names.json\"),\n",
        "          orient=\"records\"\n",
        "      )\n",
        "\n",
        "      # 4. Gefilterte Daten\n",
        "      if self.data_filtered is not None:\n",
        "          self.data_filtered.to_parquet(\n",
        "             os.path.join(storagepath, \"filtered_data.parquet\")\n",
        "          )\n",
        "\n",
        "    @classmethod\n",
        "    def get(cls, storagepath):\n",
        "        \"\"\"Lädt gespeicherte Instanz\"\"\"\n",
        "        instance = cls.__new__(cls)\n",
        "        instance.data_path = None  # Nicht mehr relevant\n",
        "\n",
        "        # 1. Modell laden\n",
        "        instance.model = BPR.load(\n",
        "            path=storagepath,\n",
        "            model_name=\"BPR_model\",\n",
        "            data_info=None  # Wird automatisch geladen\n",
        "        )\n",
        "\n",
        "        # 2. DataInfo aus dem Modell holen\n",
        "        instance.data_info = instance.model.data_info\n",
        "\n",
        "        # 3. User-Mapping laden\n",
        "        with open(os.path.join(storagepath, \"user_mapping.json\"), \"r\") as f:\n",
        "            instance.user_id_map = json.load(f)\n",
        "\n",
        "        # 4. Rezeptnamen\n",
        "        instance.name_df = pd.read_json(\n",
        "            os.path.join(storagepath, \"recipe_names.json\"),\n",
        "            orient=\"records\"\n",
        "        )\n",
        "\n",
        "        # 5. Gefilterte Daten\n",
        "        instance.data_filtered = pd.read_parquet(\n",
        "            os.path.join(storagepath, \"filtered_data.parquet\")\n",
        "        )\n",
        "\n",
        "        return instance\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def _load_recipe_names(self):\n",
        "        \"\"\"Load recipe ID to name mapping\"\"\"\n",
        "        path = kagglehub.dataset_download(self.data_path)\n",
        "        raw_recipes_path = os.path.join(path, \"RAW_recipes.csv\")\n",
        "        self.name_df = pd.read_csv(raw_recipes_path)[[\"name\", \"id\"]]\n",
        "\n",
        "    def _rename_and_filter_data(self, interactions_data):\n",
        "      # Erzeuge explizite Kopie des DataFrames\n",
        "      df = interactions_data.copy()\n",
        "\n",
        "      # Spalten umbenennen (ohne inplace)\n",
        "      df = df.rename(columns={\n",
        "          \"user_id\": \"user\",\n",
        "          \"recipe_id\": \"item\",\n",
        "          \"rating\": \"label\"\n",
        "      })\n",
        "\n",
        "      # Spalten filtern\n",
        "      keep_cols = [\"user\", \"item\", \"label\"]\n",
        "      df = df[keep_cols]\n",
        "\n",
        "      # Typkonvertierung mit .loc\n",
        "      df.loc[:, \"label\"] = df[\"label\"].astype(int)\n",
        "      return df\n",
        "\n",
        "    def _get_user_interactions(self, user_id):\n",
        "      \"\"\"Get recipes rated by a user\"\"\"\n",
        "      df = self.data_filtered[self.data_filtered['user'] == user_id]\n",
        "      for _, row in df.iterrows():\n",
        "         recipe = self._get_recipe_name(row['item'])\n",
        "         rating = row['label']\n",
        "         print(f\"Recipe: {recipe}, Rating: {rating}\")\n",
        "\n",
        "\n",
        "    def _get_recipe_name(self, recipe_id):\n",
        "        \"\"\"Helper to get recipe name from ID\"\"\"\n",
        "        name = self.name_df.loc[self.name_df['id'] == recipe_id, 'name']\n",
        "        return name.values[0] if not name.empty else \"Unknown Recipe\"\n",
        "\n",
        "    def import_ratings_csv(self, file_path):\n",
        "      \"\"\"Import ratings from CSV and map UUIDs to numeric IDs\"\"\"\n",
        "      try:\n",
        "          # Load CSV\n",
        "          df = pd.read_csv(file_path)\n",
        "          df = df.drop(columns=['item_title', 'item_ingredients'], errors='ignore')\n",
        "\n",
        "          # Check required columns\n",
        "          required = {\"uuid\", \"item_id\", \"rating\"}\n",
        "          if not required.issubset(df.columns):\n",
        "              missing = required - set(df.columns)\n",
        "              raise ValueError(f\"Fehlende Spalten: {missing}\")\n",
        "\n",
        "          # Process and map UUIDs\n",
        "          processed_df = self.__process_ratings(df)\n",
        "\n",
        "          # Add to data\n",
        "          self.data_filtered = pd.concat(\n",
        "              [self.data_filtered, processed_df],\n",
        "              ignore_index=True\n",
        "         )\n",
        "          print(f\"{len(processed_df)} neue Bewertungen hinzugefügt.\")\n",
        "\n",
        "      except FileNotFoundError:\n",
        "          print(f\"Datei {file_path} nicht gefunden.\")\n",
        "      except Exception as e:\n",
        "          print(f\"Fehler: {str(e)}\")\n",
        "\n",
        "    def __process_ratings(self, df):\n",
        "      \"\"\"Map UUIDs to numeric IDs\"\"\"\n",
        "      # Rename columns\n",
        "      df = df.rename(columns={\n",
        "          \"uuid\": \"user\",\n",
        "          \"item_id\": \"item\",\n",
        "          \"rating\": \"label\"\n",
        "      })\n",
        "\n",
        "      # Convert from range [-2,2] to [1,5]\n",
        "      df[\"label\"] = df[\"label\"] + 3\n",
        "\n",
        "      # Determine current max ID from user_id_map\n",
        "      current_max = max(self.user_id_map.values()) if self.user_id_map else 0\n",
        "\n",
        "      # Generate new IDs for unknown UUIDs\n",
        "      new_users = [uuid for uuid in df[\"user\"].unique() if uuid not in self.user_id_map]\n",
        "      num_new = len(new_users)\n",
        "\n",
        "      if num_new > 0:\n",
        "          new_ids = range(current_max + 1, current_max + num_new + 1)\n",
        "          self.user_id_map.update(zip(new_users, new_ids))\n",
        "\n",
        "      print(\"CSV erfolgreich geladen:\")\n",
        "      print(df.head())\n",
        "\n",
        "      # Replace UUIDs with numeric IDs\n",
        "      df[\"user\"] = df[\"user\"].map(self.user_id_map)\n",
        "\n",
        "      return df\n",
        "\n",
        "\n",
        "    def __get_score(self,userid,itemid):\n",
        "     return self.model.predict(userid,itemid)\n",
        "\n",
        "    def __find_item_by_id(self,recipe_id, items_information):\n",
        "      df = items_information.loc[items_information[\"id\"] == recipe_id]\n",
        "      return df['name'].values[0], df['ingredients'].values[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Beispielaufruf\n",
        "recommender = RecipeRecommender()\n",
        "recommender.load_and_preprocess(min_interactions=20)\n",
        "# Neue Nutzer per CSV importieren\n",
        "recommender.import_ratings_csv(\"/content/sample_data/ratings.csv\")\n",
        "recommender.set_model(\"BPR\") # \"SVD\", \"SVDpp\", \"ALS\", \"BPR\", \"UserCF\", \"ItemCF\"\n",
        "recommender.train()\n",
        "recommender.evaluate()"
      ],
      "metadata": {
        "id": "Ou5fJq46b9og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_items_information():\n",
        "    path = kagglehub.dataset_download(\"shuyangli94/food-com-recipes-and-user-interactions\")\n",
        "\n",
        "    recipes_path = os.path.join(path, \"RAW_recipes.csv\")\n",
        "    recipes = pd.read_csv(recipes_path)\n",
        "\n",
        "    return recipes\n",
        "\n",
        "items_information = load_items_information()"
      ],
      "metadata": {
        "id": "QWp6kdNpY1zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "from config import RATINGS_FILE\n",
        "from config import RECOMMENDATIONS_FILE\n",
        "import io\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "RECOMMENDER_TYPES = [\"SVD\", \"SVDpp\", \"ALS\", \"BPR\", \"UserCF\", \"ItemCF\"]\n",
        "\n",
        "for RECOMMENDER_TYPE in RECOMMENDER_TYPES:\n",
        "    print(f\"Training {RECOMMENDER_TYPE}...\")\n",
        "\n",
        "    with contextlib.redirect_stdout(io.StringIO()): # suppresses print statements\n",
        "        recommender = RecipeRecommender()\n",
        "        recommender.load_and_preprocess(min_interactions=20)\n",
        "        # Neue Nutzer per CSV importieren\n",
        "        recommender.import_ratings_csv(\"../\"+RATINGS_FILE)\n",
        "        recommender.set_model(RECOMMENDER_TYPE)\n",
        "        recommender.train()\n",
        "        eval = recommender.evaluate()\n",
        "\n",
        "    print(eval)\n",
        "    recommendations = recommender.save_recommendations_as_csv(\n",
        "        items_information,\n",
        "        20,\n",
        "        \"../\"+RECOMMENDATIONS_FILE+RECOMMENDER_TYPE.lower()+\".csv\"\n",
        "    )\n",
        "\n",
        "    print(f\"Recommendations for {RECOMMENDER_TYPE} saved to {RECOMMENDATIONS_FILE+RECOMMENDER_TYPE.lower()+'.csv'}\")\n",
        ""
      ],
      "metadata": {
        "id": "GjK7xMVO6c2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}