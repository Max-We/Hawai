{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# !pip install pandas\n",
    "# !pip install --upgrade kagglehub\n",
    "# !pip install -U LibRecommender\n",
    "# !pip install keras==2.12.0 tensorflow==2.12.0\n",
    "#\n",
    "# !pip show LibRecommender"
   ],
   "metadata": {
    "id": "gS5ZkBCrWQOY",
    "ExecuteTime": {
     "end_time": "2025-03-27T16:53:58.294397Z",
     "start_time": "2025-03-27T16:53:58.288891Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kDA15rXbWIoB",
    "ExecuteTime": {
     "end_time": "2025-03-27T16:54:02.217260Z",
     "start_time": "2025-03-27T16:53:58.339945Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from libreco.data import random_split, DatasetPure\n",
    "from libreco.algorithms import BPR\n",
    "from libreco.evaluation import evaluate\n",
    "import kagglehub\n",
    "import tensorflow as tf\n",
    "\n",
    "class RecipeRecommender:\n",
    "    def __init__(self, data_path=\"shuyangli94/food-com-recipes-and-user-interactions\"):\n",
    "        self.data_path = data_path\n",
    "        self.model = None\n",
    "        self.data_info = None\n",
    "        self.name_df = None\n",
    "        self.data_filtered = None\n",
    "        self.train_data = None\n",
    "        self.eval_data = None\n",
    "        self.test_data = None\n",
    "        self.user_id_map = {}\n",
    "        self._load_recipe_names()\n",
    "\n",
    "        # Initialize recipe name mapping\n",
    "        self._load_recipe_names()\n",
    "\n",
    "\n",
    "    def load_and_preprocess(self, min_interactions):\n",
    "        \"\"\"Load and preprocess interaction data\"\"\"\n",
    "        # Download and load dataset\n",
    "        path = kagglehub.dataset_download(self.data_path)\n",
    "\n",
    "        # Load and combine interaction data\n",
    "        train = pd.read_csv(os.path.join(path, \"interactions_train.csv\"))\n",
    "        eval = pd.read_csv(os.path.join(path, \"interactions_validation.csv\"))\n",
    "        test = pd.read_csv(os.path.join(path, \"interactions_test.csv\"))\n",
    "\n",
    "        combined = pd.concat([train, eval, test], ignore_index=True)\n",
    "        combined = self._rename_and_filter_data(combined)\n",
    "\n",
    "        # Filter items\n",
    "        item_counts = combined[\"item\"].value_counts()\n",
    "        items_to_keep = item_counts[item_counts >= min_interactions].index\n",
    "        filtered = combined[combined[\"item\"].isin(items_to_keep)]\n",
    "\n",
    "        # Filter users\n",
    "        user_counts = filtered[\"user\"].value_counts()\n",
    "        users_to_keep = user_counts[user_counts >= min_interactions].index\n",
    "        self.data_filtered = filtered[filtered[\"user\"].isin(users_to_keep)]\n",
    "\n",
    "    def train(self, embed_size=256, n_epochs=5, lr=5e-5):\n",
    "        \"\"\"Train the recommendation model\"\"\"\n",
    "        # Split data\n",
    "        self.train_data, self.eval_data, self.test_data = random_split(\n",
    "            self.data_filtered,\n",
    "            multi_ratios=[0.8, 0.1, 0.1]\n",
    "        )\n",
    "\n",
    "        # Build datasets\n",
    "        self.train_data, self.data_info = DatasetPure.build_trainset(self.train_data)\n",
    "        self.eval_data = DatasetPure.build_evalset(self.eval_data)\n",
    "        self.test_data = DatasetPure.build_testset(self.test_data)\n",
    "\n",
    "        # Initialize model\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        self.model = BPR(\n",
    "            task=\"ranking\",\n",
    "            data_info=self.data_info,\n",
    "            loss_type=\"bpr\",\n",
    "            embed_size=embed_size,\n",
    "            n_epochs=n_epochs,\n",
    "            lr=lr,\n",
    "            batch_size=1024,\n",
    "            num_neg=5,\n",
    "            reg=5e-6,\n",
    "            sampler=\"random\"\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        self.model.fit(\n",
    "            self.train_data,\n",
    "            neg_sampling=True,\n",
    "            shuffle=True,\n",
    "            verbose=2,\n",
    "            eval_data=self.eval_data,\n",
    "            metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"]\n",
    "        )\n",
    "    def save_recommendations_as_csv(self,items_information,amount_of_recs, path):\n",
    "      df = self.get_recommendations(items_information,amount_of_recs)\n",
    "      df.to_csv(path, index=False)\n",
    "      return df\n",
    "\n",
    "    def get_recommendations(self, items_information, n_rec):\n",
    "        \"\"\"\n",
    "        Holt Empfehlungen für alle User in user_id_map und speichert die Ergebnisse in einem DataFrame.\n",
    "        \"\"\"\n",
    "        dfs = []\n",
    "        for user_identifier in self.user_id_map:\n",
    "            df = self.get_recommendation(user_identifier, n_rec, items_information)\n",
    "            dfs.append(df)\n",
    "        # Alle einzelnen DataFrames zusammenfügen\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        return final_df\n",
    "\n",
    "    def get_recommendation(self, user_identifier, n_rec, items_information):\n",
    "      \"\"\"Get recommendations for a user (UUID or numeric ID) und speichert alle Daten in einem DataFrame\"\"\"\n",
    "      if not self.model:\n",
    "          raise ValueError(\"Model not trained. Call train() first.\")\n",
    "\n",
    "      # UUID Lookup\n",
    "      if isinstance(user_identifier, str):\n",
    "          if user_identifier not in self.user_id_map:\n",
    "              raise ValueError(f\"User UUID '{user_identifier}' not found.\")\n",
    "          user_id = self.user_id_map[user_identifier]\n",
    "\n",
    "     # Empfehlungen abrufen\n",
    "      recommendations = self.model.recommend_user(\n",
    "          user=user_id,\n",
    "          n_rec=n_rec,\n",
    "          filter_consumed=True\n",
    "     )\n",
    "\n",
    "      # Liste für die Daten vorbereiten\n",
    "      records = []\n",
    "      for recipe in recommendations[user_id]:\n",
    "          # Item-Titel und Zutaten anhand der recipe_id abrufen\n",
    "          item_title, item_ingredients = self.__find_item_by_id(recipe, items_information)\n",
    "          # Datensatz zur Liste hinzufügen\n",
    "          records.append({\n",
    "              \"uuid\": user_identifier,\n",
    "              \"item_id\": recipe,\n",
    "             \"item_title\": item_title,\n",
    "              \"item_ingredients\": item_ingredients\n",
    "          })\n",
    "\n",
    "      # DataFrame aus der Liste erstellen\n",
    "      df = pd.DataFrame(records)\n",
    "      return df\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        return evaluate(\n",
    "            model=self.model,\n",
    "            data=self.test_data,\n",
    "            neg_sampling=True,\n",
    "            metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"]\n",
    "        )\n",
    "\n",
    "    def info(self, UUID):\n",
    "      \"\"\"Gibt einen DataFrame mit allen Interaktionen des angegebenen Benutzers (UUID) zurück.\"\"\"\n",
    "      # Überprüfen, ob Daten geladen wurden\n",
    "      if self.data_filtered is None or not isinstance(self.data_filtered, pd.DataFrame):\n",
    "          return pd.DataFrame(columns=[\"user\", \"item\", \"label\", \"name\"])\n",
    "\n",
    "      # Prüfen, ob die UUID vorhanden ist\n",
    "      if UUID not in self.user_id_map:\n",
    "          return pd.DataFrame(columns=[\"user\", \"item\", \"label\", \"name\"])\n",
    "\n",
    "      # Numerische Benutzer-ID abrufen\n",
    "      user_id = self.user_id_map[UUID]\n",
    "\n",
    "      # Interaktionen filtern\n",
    "      user_interactions = self.data_filtered[self.data_filtered['user'] == user_id].copy()\n",
    "\n",
    "      if user_interactions.empty:\n",
    "          return pd.DataFrame(columns=[\"user\", \"item\", \"label\", \"name\"])\n",
    "\n",
    "      # UUID statt numerischer ID setzen\n",
    "      user_interactions['user'] = UUID\n",
    "\n",
    "      # Rezeptnamen hinzufügen\n",
    "      merged = user_interactions.merge(self.name_df, left_on='item', right_on='id', how='left')\n",
    "      merged['name'] = merged['name'].fillna('Unknown Recipe')\n",
    "\n",
    "      # Ergebnis formatieren\n",
    "      result = merged[['user', 'item', 'label', 'name']]\n",
    "\n",
    "      return result\n",
    "\n",
    "    def save(self, storagepath):\n",
    "      \"\"\"Speichert Modell und Zustand\"\"\"\n",
    "      if not self.model:\n",
    "          raise ValueError(\"Modell nicht trainiert\")\n",
    "\n",
    "      os.makedirs(storagepath, exist_ok=True)\n",
    "\n",
    "      # 1. Modell mit LibreCos eigener Methode speichern\n",
    "      self.model.save(storagepath, model_name=\"BPR_model\")\n",
    "\n",
    "      # 2. User-Mapping als JSON\n",
    "      with open(os.path.join(storagepath, \"user_mapping.json\"), \"w\") as f:\n",
    "          json.dump(self.user_id_map, f)\n",
    "\n",
    "      # 3. Rezeptnamen-Daten\n",
    "      self.name_df.to_json(\n",
    "          os.path.join(storagepath, \"recipe_names.json\"),\n",
    "          orient=\"records\"\n",
    "      )\n",
    "\n",
    "      # 4. Gefilterte Daten\n",
    "      if self.data_filtered is not None:\n",
    "          self.data_filtered.to_parquet(\n",
    "             os.path.join(storagepath, \"filtered_data.parquet\")\n",
    "          )\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, storagepath):\n",
    "        \"\"\"Lädt gespeicherte Instanz\"\"\"\n",
    "        instance = cls.__new__(cls)\n",
    "        instance.data_path = None  # Nicht mehr relevant\n",
    "\n",
    "        # 1. Modell laden\n",
    "        instance.model = BPR.load(\n",
    "            path=storagepath,\n",
    "            model_name=\"BPR_model\",\n",
    "            data_info=None  # Wird automatisch geladen\n",
    "        )\n",
    "\n",
    "        # 2. DataInfo aus dem Modell holen\n",
    "        instance.data_info = instance.model.data_info\n",
    "\n",
    "        # 3. User-Mapping laden\n",
    "        with open(os.path.join(storagepath, \"user_mapping.json\"), \"r\") as f:\n",
    "            instance.user_id_map = json.load(f)\n",
    "\n",
    "        # 4. Rezeptnamen\n",
    "        instance.name_df = pd.read_json(\n",
    "            os.path.join(storagepath, \"recipe_names.json\"),\n",
    "            orient=\"records\"\n",
    "        )\n",
    "\n",
    "        # 5. Gefilterte Daten\n",
    "        instance.data_filtered = pd.read_parquet(\n",
    "            os.path.join(storagepath, \"filtered_data.parquet\")\n",
    "        )\n",
    "\n",
    "        return instance\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def _load_recipe_names(self):\n",
    "        \"\"\"Load recipe ID to name mapping\"\"\"\n",
    "        path = kagglehub.dataset_download(self.data_path)\n",
    "        raw_recipes_path = os.path.join(path, \"RAW_recipes.csv\")\n",
    "        self.name_df = pd.read_csv(raw_recipes_path)[[\"name\", \"id\"]]\n",
    "\n",
    "    def _rename_and_filter_data(self, interactions_data):\n",
    "      # Erzeuge explizite Kopie des DataFrames\n",
    "      df = interactions_data.copy()\n",
    "\n",
    "      # Spalten umbenennen (ohne inplace)\n",
    "      df = df.rename(columns={\n",
    "          \"user_id\": \"user\",\n",
    "          \"recipe_id\": \"item\",\n",
    "          \"rating\": \"label\"\n",
    "      })\n",
    "\n",
    "      # Spalten filtern\n",
    "      keep_cols = [\"user\", \"item\", \"label\"]\n",
    "      df = df[keep_cols]\n",
    "\n",
    "      # Typkonvertierung mit .loc\n",
    "      df.loc[:, \"label\"] = df[\"label\"].astype(int)\n",
    "      return df\n",
    "\n",
    "    def _get_recipe_name(self, recipe_id):\n",
    "        \"\"\"Helper to get recipe name from ID\"\"\"\n",
    "        name = self.name_df.loc[self.name_df['id'] == recipe_id, 'name']\n",
    "        return name.values[0] if not name.empty else \"Unknown Recipe\"\n",
    "\n",
    "    def import_ratings_csv(self, file_path):\n",
    "      \"\"\"Import ratings from CSV and map UUIDs to numeric IDs\"\"\"\n",
    "      try:\n",
    "          # Load CSV\n",
    "          df = pd.read_csv(file_path)\n",
    "          print(\"CSV erfolgreich geladen:\")\n",
    "          print(df.head())\n",
    "\n",
    "          # Check required columns\n",
    "          required = {\"uuid\", \"item_id\", \"rating\"}\n",
    "          if not required.issubset(df.columns):\n",
    "              missing = required - set(df.columns)\n",
    "              raise ValueError(f\"Fehlende Spalten: {missing}\")\n",
    "\n",
    "          # Process and map UUIDs\n",
    "          processed_df = self.__process_ratings(df)\n",
    "\n",
    "          # Add to data\n",
    "          self.data_filtered = pd.concat(\n",
    "              [self.data_filtered, processed_df],\n",
    "              ignore_index=True\n",
    "         )\n",
    "          print(f\"{len(processed_df)} neue Bewertungen hinzugefügt.\")\n",
    "\n",
    "      except FileNotFoundError:\n",
    "          print(f\"Datei {file_path} nicht gefunden.\")\n",
    "      except Exception as e:\n",
    "          print(f\"Fehler: {str(e)}\")\n",
    "\n",
    "    def __process_ratings(self, df):\n",
    "      \"\"\"Map UUIDs to numeric IDs\"\"\"\n",
    "      # Rename columns\n",
    "      df = df.rename(columns={\n",
    "          \"uuid\": \"user\",\n",
    "          \"item_id\": \"item\",\n",
    "          \"rating\": \"label\"\n",
    "      })\n",
    "\n",
    "      # convert rating value range from csv (-2,2) to (1,5)\n",
    "      df[\"label\"] = df[\"label\"].apply(lambda x: x+3)\n",
    "\n",
    "      # Determine current max ID from user_id_map\n",
    "      current_max = max(self.user_id_map.values()) if self.user_id_map else 0\n",
    "\n",
    "      # Generate new IDs for unknown UUIDs\n",
    "      new_users = [uuid for uuid in df[\"user\"].unique() if uuid not in self.user_id_map]\n",
    "      num_new = len(new_users)\n",
    "\n",
    "      if num_new > 0:\n",
    "          new_ids = range(current_max + 1, current_max + num_new + 1)\n",
    "          self.user_id_map.update(zip(new_users, new_ids))\n",
    "\n",
    "      # Replace UUIDs with numeric IDs\n",
    "      df[\"user\"] = df[\"user\"].map(self.user_id_map)\n",
    "      return df\n",
    "\n",
    "\n",
    "    def __get_score(self,userid,itemid):\n",
    "     return self.model.predict(userid,itemid)\n",
    "\n",
    "    def __find_item_by_id(self,recipe_id, items_information):\n",
    "      df = items_information.loc[items_information[\"id\"] == recipe_id]\n",
    "      return df['name'].values[0], df['ingredients'].values[0]\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 17:53:59.078960: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-27 17:53:59.080710: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-27 17:53:59.113755: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-27 17:53:59.114606: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-27 17:53:59.664469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mw/anaconda3/envs/repr-u1-10/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mw/anaconda3/envs/repr-u1-10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "from config import RATINGS_FILE\n",
    "\n",
    "# Beispielaufruf\n",
    "recommender = RecipeRecommender()\n",
    "recommender.load_and_preprocess(min_interactions=20)\n",
    "# Neue Nutzer per CSV importieren\n",
    "recommender.import_ratings_csv(\"../\"+RATINGS_FILE)\n",
    "recommender.train()\n",
    "recommender.evaluate()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ou5fJq46b9og",
    "outputId": "d8570dd8-4973-4f4c-fb6e-42cc130161d6",
    "ExecuteTime": {
     "end_time": "2025-03-27T16:54:32.117222Z",
     "start_time": "2025-03-27T16:54:02.284139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV erfolgreich geladen:\n",
      "                                   uuid  item_id  rating  \\\n",
      "0  3593cd68-32a3-4575-a41e-03d94d968649    90323       1   \n",
      "1  3593cd68-32a3-4575-a41e-03d94d968649    43806       2   \n",
      "2  3593cd68-32a3-4575-a41e-03d94d968649    41168      -1   \n",
      "3  3593cd68-32a3-4575-a41e-03d94d968649    90229       1   \n",
      "4  3593cd68-32a3-4575-a41e-03d94d968649      519       1   \n",
      "\n",
      "                                          item_title  \\\n",
      "0                                        garlic loaf   \n",
      "1  chicken coconut curry soup   a k a  easy mulli...   \n",
      "2                       cherry almond butter cookies   \n",
      "3                                garlic fries  light   \n",
      "4              second only to my meatloaf  meatballs   \n",
      "\n",
      "                                    item_ingredients  \n",
      "0  ['butter', 'parsley', 'garlic powder', 'garlic...  \n",
      "1  ['whole boneless skinless chicken breast', 'po...  \n",
      "2  ['butter', 'sugar', 'egg', 'vanilla', 'flour',...  \n",
      "3  ['canola oil', 'salt', 'baking potatoes', 'coo...  \n",
      "4  ['ground beef', 'oatmeal', 'onion', 'egg', 'mi...  \n",
      "500 neue Bewertungen hinzugefügt.\n",
      "Training start time: \u001B[35m2025-03-27 17:54:09\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 17:54:09.804373: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "train: 100%|██████████| 518/518 [00:03<00:00, 170.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 elapsed: 3.032s\n",
      "\t \u001B[32mtrain_loss: 0.6923\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 209.47it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:00<00:00, 3081.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6926\n",
      "\t eval roc_auc: 0.5482\n",
      "\t eval precision@10: 0.0045\n",
      "\t eval recall@10: 0.0088\n",
      "\t eval ndcg@10: 0.0220\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 518/518 [00:02<00:00, 172.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 elapsed: 3.004s\n",
      "\t \u001B[32mtrain_loss: 0.6905\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 196.97it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:00<00:00, 3259.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6920\n",
      "\t eval roc_auc: 0.5790\n",
      "\t eval precision@10: 0.0089\n",
      "\t eval recall@10: 0.0169\n",
      "\t eval ndcg@10: 0.0421\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 518/518 [00:02<00:00, 173.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 elapsed: 2.988s\n",
      "\t \u001B[32mtrain_loss: 0.6885\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 203.77it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:00<00:00, 3173.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6915\n",
      "\t eval roc_auc: 0.6021\n",
      "\t eval precision@10: 0.0113\n",
      "\t eval recall@10: 0.0219\n",
      "\t eval ndcg@10: 0.0527\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 518/518 [00:03<00:00, 166.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 elapsed: 3.109s\n",
      "\t \u001B[32mtrain_loss: 0.6866\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 193.07it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:00<00:00, 3613.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6909\n",
      "\t eval roc_auc: 0.6185\n",
      "\t eval precision@10: 0.0132\n",
      "\t eval recall@10: 0.0262\n",
      "\t eval ndcg@10: 0.0604\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 518/518 [00:03<00:00, 164.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 elapsed: 3.149s\n",
      "\t \u001B[32mtrain_loss: 0.6847\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 184.20it/s]\n",
      "eval_listwise: 100%|██████████| 2426/2426 [00:01<00:00, 1636.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6904\n",
      "\t eval roc_auc: 0.6301\n",
      "\t eval precision@10: 0.0141\n",
      "\t eval recall@10: 0.0283\n",
      "\t eval ndcg@10: 0.0656\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 10/10 [00:00<00:00, 177.88it/s]\n",
      "eval_listwise: 100%|██████████| 2401/2401 [00:01<00:00, 1895.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.6903925517189129,\n",
       " 'roc_auc': 0.6317725844049902,\n",
       " 'precision': 0.015243648479800084,\n",
       " 'recall': 0.03151605799500543,\n",
       " 'ndcg': 0.06885874571273692}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "ITc2b4S26zz_",
    "outputId": "95cdd1b6-b6ac-420b-941b-ec0721f839e1",
    "ExecuteTime": {
     "end_time": "2025-03-27T16:54:32.173300Z",
     "start_time": "2025-03-27T16:54:32.165020Z"
    }
   },
   "cell_type": "code",
   "source": "recommender.info(\"df903ba4-a8f3-406f-814d-4420b00ab611\")",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user, item, label, name]\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "def load_items_information():\n",
    "    path = kagglehub.dataset_download(\"shuyangli94/food-com-recipes-and-user-interactions\")\n",
    "\n",
    "    recipes_path = os.path.join(path, \"RAW_recipes.csv\")\n",
    "    recipes = pd.read_csv(recipes_path)\n",
    "\n",
    "    return recipes\n",
    "\n",
    "items_information = load_items_information()"
   ],
   "metadata": {
    "id": "QWp6kdNpY1zs",
    "ExecuteTime": {
     "end_time": "2025-03-27T16:54:35.371812Z",
     "start_time": "2025-03-27T16:54:32.368750Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "id": "SRTFdV-BtIMu",
    "ExecuteTime": {
     "end_time": "2025-03-27T16:54:35.635623Z",
     "start_time": "2025-03-27T16:54:35.376848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config import RECOMMENDATIONS_FILE\n",
    "\n",
    "# Empfehlungen für importierten Nutzer (UUID)\n",
    "recommendations = recommender.save_recommendations_as_csv(items_information,20, \"../\"+RECOMMENDATIONS_FILE+\"bpr.csv\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "HCtoWg3Eyw2D",
    "outputId": "862aa672-b2d7-41a3-b442-49bc57f25adc",
    "ExecuteTime": {
     "end_time": "2025-03-27T16:54:35.671391Z",
     "start_time": "2025-03-27T16:54:35.660106Z"
    }
   },
   "cell_type": "code",
   "source": "recommendations",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                     uuid  item_id  \\\n",
       "0    3593cd68-32a3-4575-a41e-03d94d968649    89204   \n",
       "1    3593cd68-32a3-4575-a41e-03d94d968649    68955   \n",
       "2    3593cd68-32a3-4575-a41e-03d94d968649    28148   \n",
       "3    3593cd68-32a3-4575-a41e-03d94d968649    39087   \n",
       "4    3593cd68-32a3-4575-a41e-03d94d968649    27208   \n",
       "..                                    ...      ...   \n",
       "495  0feaada3-c3e5-4b9f-a8d9-6fd5930985da    43023   \n",
       "496  0feaada3-c3e5-4b9f-a8d9-6fd5930985da    28148   \n",
       "497  0feaada3-c3e5-4b9f-a8d9-6fd5930985da    69173   \n",
       "498  0feaada3-c3e5-4b9f-a8d9-6fd5930985da    34382   \n",
       "499  0feaada3-c3e5-4b9f-a8d9-6fd5930985da    90975   \n",
       "\n",
       "                                            item_title  \\\n",
       "0    crock pot chicken with black beans   cream cheese   \n",
       "1                               japanese mum s chicken   \n",
       "2                      oven fried chicken chimichangas   \n",
       "3                           creamy cajun chicken pasta   \n",
       "4                           to die for crock pot roast   \n",
       "..                                                 ...   \n",
       "495                          creamy garlic penne pasta   \n",
       "496                    oven fried chicken chimichangas   \n",
       "497   kittencal s italian melt in your mouth meatballs   \n",
       "498       mashed red potatoes with garlic and parmesan   \n",
       "499                           the ultimate greek salad   \n",
       "\n",
       "                                      item_ingredients  \n",
       "0    ['boneless chicken breasts', 'black beans', 'c...  \n",
       "1    ['chicken drumsticks', 'water', 'balsamic vine...  \n",
       "2    ['picante sauce', 'ground cumin', 'dried orega...  \n",
       "3    ['boneless skinless chicken breast halves', 'l...  \n",
       "4    ['beef roast', 'brown gravy mix', 'dried itali...  \n",
       "..                                                 ...  \n",
       "495  ['penne', 'butter', 'garlic cloves', 'flour', ...  \n",
       "496  ['picante sauce', 'ground cumin', 'dried orega...  \n",
       "497  ['ground beef', 'egg', 'parmesan cheese', 'bre...  \n",
       "498  ['red potatoes', 'garlic cloves', 'butter', 'm...  \n",
       "499  ['olive oil', 'fresh lemon juice', 'red wine v...  \n",
       "\n",
       "[500 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_title</th>\n",
       "      <th>item_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3593cd68-32a3-4575-a41e-03d94d968649</td>\n",
       "      <td>89204</td>\n",
       "      <td>crock pot chicken with black beans   cream cheese</td>\n",
       "      <td>['boneless chicken breasts', 'black beans', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3593cd68-32a3-4575-a41e-03d94d968649</td>\n",
       "      <td>68955</td>\n",
       "      <td>japanese mum s chicken</td>\n",
       "      <td>['chicken drumsticks', 'water', 'balsamic vine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3593cd68-32a3-4575-a41e-03d94d968649</td>\n",
       "      <td>28148</td>\n",
       "      <td>oven fried chicken chimichangas</td>\n",
       "      <td>['picante sauce', 'ground cumin', 'dried orega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3593cd68-32a3-4575-a41e-03d94d968649</td>\n",
       "      <td>39087</td>\n",
       "      <td>creamy cajun chicken pasta</td>\n",
       "      <td>['boneless skinless chicken breast halves', 'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3593cd68-32a3-4575-a41e-03d94d968649</td>\n",
       "      <td>27208</td>\n",
       "      <td>to die for crock pot roast</td>\n",
       "      <td>['beef roast', 'brown gravy mix', 'dried itali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0feaada3-c3e5-4b9f-a8d9-6fd5930985da</td>\n",
       "      <td>43023</td>\n",
       "      <td>creamy garlic penne pasta</td>\n",
       "      <td>['penne', 'butter', 'garlic cloves', 'flour', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0feaada3-c3e5-4b9f-a8d9-6fd5930985da</td>\n",
       "      <td>28148</td>\n",
       "      <td>oven fried chicken chimichangas</td>\n",
       "      <td>['picante sauce', 'ground cumin', 'dried orega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0feaada3-c3e5-4b9f-a8d9-6fd5930985da</td>\n",
       "      <td>69173</td>\n",
       "      <td>kittencal s italian melt in your mouth meatballs</td>\n",
       "      <td>['ground beef', 'egg', 'parmesan cheese', 'bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0feaada3-c3e5-4b9f-a8d9-6fd5930985da</td>\n",
       "      <td>34382</td>\n",
       "      <td>mashed red potatoes with garlic and parmesan</td>\n",
       "      <td>['red potatoes', 'garlic cloves', 'butter', 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0feaada3-c3e5-4b9f-a8d9-6fd5930985da</td>\n",
       "      <td>90975</td>\n",
       "      <td>the ultimate greek salad</td>\n",
       "      <td>['olive oil', 'fresh lemon juice', 'red wine v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:54:35.754483Z",
     "start_time": "2025-03-27T16:54:35.752488Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
